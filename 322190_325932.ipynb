{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ebbf09-059d-46da-b87b-25516ca17316",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (CS-456)\n",
    "## Miniproject 1: Tic Tac Toe\n",
    "- Mickaël Achkar (322190)\n",
    "- Yehya El Hassan (325932)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "072f5258-99aa-4582-9c6c-28f49e65e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from typing import Dict, List\n",
    "import hashlib\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "40a3e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8f3025cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RlAgent():\n",
    "    def __init__(self,player:str,epsilon:float,learning_rate:float = 0.05,discount_factor:float = 0.99):\n",
    "        self.player = player\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Form the the q_table and initialize it to empty\n",
    "        self.q_table:Dict[str,np.ndarray] = {}\n",
    "\n",
    "        # Actions take values between 0 and 9 representing the possible positions on the board\n",
    "        self.list_of_possible_actions:List[int] = [] \n",
    "\n",
    "        # Initialize the current state to None and current action to None\n",
    "        self.current_state = None\n",
    "\n",
    "        # Initialize the current reward\n",
    "        self.reward = 0\n",
    "\n",
    "    def update(self, board, reward):\n",
    "        # Recieves information from the environment\n",
    "        self._update_board(board)\n",
    "        self._update_current_state(get_state_from_board(board))\n",
    "        self._update_reward(reward)\n",
    "        self._update_list_of_possible_actions()\n",
    "\n",
    "\n",
    "    def update_q_table(self,previous_state,previous_action, current_action):\n",
    "        previous_state = get_state_from_board(previous_state)\n",
    "        self.q_table[previous_state][previous_action] = self.q_table[previous_state][previous_action] + self.learning_rate*(self.reward + self.discount_factor*(self.q_table[self.current_state][current_action]))\n",
    "\n",
    "    def act(self):\n",
    "        # Sample from a uniform distribution\n",
    "        if (epsilon>random.uniform(0,1)):\n",
    "            return self._choose_random_action()\n",
    "        else:\n",
    "            return self._choose_best_action()\n",
    "\n",
    "    def _update_board(self,board):\n",
    "        # Get's the latest board configuration from the Game\n",
    "        self.board = board\n",
    "\n",
    "    def _update_current_state(self, current_state):\n",
    "        if (not isinstance(self.q_table.get(current_state), np.ndarray)):\n",
    "            self.q_table[current_state] = np.zeros(9)\n",
    "        self.current_state = current_state\n",
    "  \n",
    "    def _update_current_action(self, current_action):\n",
    "        self.current_action = current_action\n",
    "    \n",
    "    def _update_list_of_possible_actions(self):\n",
    "        # Get's the available positions on the board\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if self.board[pos] == 0:\n",
    "                avail.append(i)\n",
    "        self.list_of_possible_actions = avail\n",
    "        return self.list_of_possible_actions\n",
    "\n",
    "    def _update_reward(self, reward):\n",
    "        # Updates the current reward\n",
    "        self.reward = reward\n",
    "\n",
    "    def _choose_best_action(self):\n",
    "        maximum_q_values = np.where(self.q_table[self.current_state][self.list_of_possible_actions] == np.max(self.q_table[self.current_state][self.list_of_possible_actions]))[0]\n",
    "        random_between_max = np.random.choice(maximum_q_values)\n",
    "        return (self.list_of_possible_actions[random_between_max])\n",
    "\n",
    "    def _choose_random_action(self):\n",
    "        return np.random.choice(self.list_of_possible_actions)\n",
    "\n",
    "\n",
    "def get_state_from_board(board):\n",
    "     # Convert the Board configuration (Matrix) into a unique key for the state\n",
    "    return hashlib.sha1(board).hexdigest()\n",
    "\n",
    "\n",
    "def logger(winner, player_1, player_2):\n",
    "    print('-------------------------------------------')\n",
    "    print(f'Game end, winner is player {str(winner)}')\n",
    "    print(f'Optimal player 1 = {str(player_1)}')\n",
    "    print(f'RL Agent player 2 = {str(player_2)}')\n",
    "    \n",
    "def choose_players(index):\n",
    "    if index%2 == 0:\n",
    "        player_1 = 'X'\n",
    "        player_2 = 'O'\n",
    "    else:\n",
    "        player_1 = 'O'\n",
    "        player_2 = 'X' \n",
    "        \n",
    "    return player_1,player_2   \n",
    "\n",
    "def initialize_rl_moves():\n",
    "    number_of_moves_of_rl_agent = 0\n",
    "    return number_of_moves_of_rl_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc958b",
   "metadata": {},
   "source": [
    "Implementing a Tic Tac Toc Player using Q-learning. With that, we will create a Q-table representing all the states and actions possible and we will progressively update the values in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3eaa6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent(environment: TictactoeEnv,number_of_episodes: int, optimal_level : float, epsilon:float, verbose: bool = False):\n",
    "    number_of_rl_wins = 0\n",
    "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
    "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"O\")\n",
    "    for i in tqdm(range(number_of_episodes)):\n",
    "        \n",
    "        environment.reset()\n",
    "        grid, _, __ = environment.observe()\n",
    "\n",
    "        player_1,player_2 = choose_players(index = i)\n",
    "\n",
    "        player_optimal.player = player_1\n",
    "        player_rl_agent.player = player_2\n",
    "        \n",
    "        number_of_moves_of_rl_agent = initialize_rl_moves()\n",
    "\n",
    "        for j in range(9):\n",
    "            player_rl_agent.update(grid,environment.reward(player=player_2))\n",
    "\n",
    "            if environment.current_player == player_optimal.player:\n",
    "                move = player_optimal.act(grid)\n",
    "            else:\n",
    "                rl_move = player_rl_agent.act()\n",
    "                number_of_moves_of_rl_agent +=1\n",
    "\n",
    "                if (number_of_moves_of_rl_agent%2 == 1):\n",
    "                    stored_move = rl_move\n",
    "                    stored_state = grid\n",
    "                    \n",
    "                elif (number_of_moves_of_rl_agent%2 == 0 and number_of_moves_of_rl_agent>0) :\n",
    "                    player_rl_agent.update_q_table(stored_state,stored_move, rl_move)\n",
    "\n",
    "                move = (int(rl_move/3),rl_move%3)\n",
    "\n",
    "            grid, end, winner = environment.step(move, print_grid=False)\n",
    "\n",
    "            if end:\n",
    "                player_rl_agent.update(grid,environment.reward(player=player_2))\n",
    "                player_rl_agent.update_q_table(stored_state,stored_move, rl_move)\n",
    "\n",
    "                if winner == player_rl_agent.player:\n",
    "                    number_of_rl_wins+=1\n",
    "                if verbose:\n",
    "                    logger(winner,player_1, player_2)\n",
    "                    environment.render()\n",
    "\n",
    "                environment.reset()\n",
    "                break\n",
    "    return player_rl_agent, number_of_rl_wins    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "58009e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:01<00:00, 326.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "epsilon = 0.1\n",
    "\n",
    "player_rl_agent, number_of_rl_wins = train_rl_agent(environment, number_of_episodes=number_of_episodes, optimal_level=0.5,epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_of_rl_wins)\n",
    "print(player_rl_agent.q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
