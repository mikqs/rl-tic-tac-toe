{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70ebbf09-059d-46da-b87b-25516ca17316",
      "metadata": {
        "id": "70ebbf09-059d-46da-b87b-25516ca17316"
      },
      "source": [
        "# Artificial Neural Networks (CS-456)\n",
        "# Miniproject 1: Tic Tac Toe\n",
        "- MickaÃ«l Achkar (322190)\n",
        "- Yehya El Hassan (325932)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "072f5258-99aa-4582-9c6c-28f49e65e087",
      "metadata": {
        "id": "072f5258-99aa-4582-9c6c-28f49e65e087"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tic_env import TictactoeEnv, OptimalPlayer\n",
        "from typing import Dict, List, Union, Callable, Tuple\n",
        "import hashlib\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import binned_statistic\n",
        "from random import randrange\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0fcc44",
      "metadata": {
        "id": "cc0fcc44"
      },
      "source": [
        "## Tabular Q-Learning\n",
        "In order to implement an agent that utilizes reinforecement learning to play Tic Tac Toe, we implemented an RLAgent class. This class handles all the needed functionalities of the RL agent such as:\n",
        "\n",
        "* `observe_state` which stores the game board and updates the list of possible actions.\n",
        "* `get_epsilon` which returns the value of epsilon. Epsilon can be either fixed or monotonically decreasing as discussed later.\n",
        "* `update_nb_of_episode_played` which updates the number of episodes played to calculate the value of the monotonically decreasing epsilon.\n",
        "* `observe_reward` which updates the internal stored value of the reward given by the environment.\n",
        "* `act` which selects an action from a list of possibled actions based on $\\epsilon$-greedy strategy.\n",
        "* `update_q_table` which stores and updates a table that contains a $Q$-value for every state and action possible in the given environment.\n",
        "\n",
        "In addition, other internal functions are denoted by a `_` prefix to implement the above discussed functionalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3025cf",
      "metadata": {
        "id": "8f3025cf"
      },
      "outputs": [],
      "source": [
        "class RlAgent():\n",
        "    def __init__(self, player: str, epsilon: Union[float, Callable], learning_rate: float = 0.05, discount_factor: float = 0.99):\n",
        "        # Choose the Player (X,O)\n",
        "        self.player = player\n",
        "\n",
        "        # Choose the exploration/exploitation factor\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # RL training hyper params\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "        # Form the the q_table and initialize it to empty\n",
        "        self.q_table: Dict[str,np.ndarray] = {}\n",
        "\n",
        "        # Store all observed states for heat map generation\n",
        "        self.observed_states: List[np.ndarray] = []\n",
        "\n",
        "        # Actions take values between 0 and 9 representing the possible positions on the board\n",
        "        self.list_of_possible_actions: List[int] = [] \n",
        "\n",
        "        # Update current episode number\n",
        "        self.current_episode = 0\n",
        "\n",
        "        # Update the is_test flag\n",
        "        self.is_test = False\n",
        "    \n",
        "    def test(self):\n",
        "        # set to testing behavior\n",
        "        self.is_test = True\n",
        "    \n",
        "    def train(self):\n",
        "        # set to training behavior\n",
        "        self.is_test = False\n",
        "\n",
        "    def observe_state(self, board, store_state:bool = False):\n",
        "        # Observe the current state of the environment\n",
        "        if store_state:\n",
        "            self.observed_states.append(board)\n",
        "        self._update_board(board)\n",
        "        self._update_list_of_possible_actions()\n",
        "\n",
        "\n",
        "    def update_nb_of_episode_played(self, episode_number):\n",
        "        self.current_episode = episode_number\n",
        "\n",
        "    def get_epsilon(self):\n",
        "        # Retun the current epsilon\n",
        "        if (isinstance(self.epsilon, Callable)):\n",
        "            return self.epsilon(self.current_episode)\n",
        "        else:\n",
        "            return self.epsilon\n",
        "\n",
        "    def act(self, board: np.ndarray):\n",
        "        # TODO: check if passing the board is needed.\n",
        "        current_state = self.get_state_key(board)\n",
        "\n",
        "        if (not self.is_test):\n",
        "            # Sample from a uniform distribution\n",
        "            if (random.uniform(0,1)<self.get_epsilon()):\n",
        "                return self._choose_random_action()\n",
        "            else:\n",
        "                return self._choose_best_action(current_state)\n",
        "        else:\n",
        "            return self._choose_best_action(current_state)\n",
        "\n",
        "    def update_q_table(self, current_board_config, current_action, current_reward, next_board_config, terminal_state = False):\n",
        "        # update the q_table\n",
        "        current_state = self.get_state_key(current_board_config)\n",
        "        next_state = self.get_state_key(next_board_config)\n",
        "\n",
        "        # create new entries if needed\n",
        "        self._create_new_state_entries_if_needed(current_state)\n",
        "        self._create_new_state_entries_if_needed(next_state)\n",
        "\n",
        "        if not terminal_state:\n",
        "            best_action = self._choose_best_action(next_state)\n",
        "            self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(current_reward + self.discount_factor*(self.q_table[next_state][best_action]) - self.q_table[current_state][current_action])\n",
        "        else:\n",
        "            self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(current_reward - self.q_table[current_state][current_action])\n",
        "\n",
        "    def _update_board(self, board):\n",
        "        # Get's the latest board configuration from the Game\n",
        "        self.board = board\n",
        "    \n",
        "    def _update_list_of_possible_actions(self):\n",
        "        # Get's the available positions on the board\n",
        "        available_actions = []\n",
        "        for i in range(9):\n",
        "            pos = (int(i/3), i % 3)\n",
        "            if self.board[pos] == 0:\n",
        "                available_actions.append(i)\n",
        "        self.list_of_possible_actions = available_actions\n",
        "        return self.list_of_possible_actions\n",
        "\n",
        "    def _choose_best_action(self, current_state):\n",
        "        self._create_new_state_entries_if_needed(current_state)\n",
        "        maximum_q_values_idx = np.where(self.q_table[current_state][self.list_of_possible_actions] == np.max(self.q_table[current_state][self.list_of_possible_actions]))[0]\n",
        "        random_max = np.random.choice(maximum_q_values_idx)\n",
        "        return (self.list_of_possible_actions[random_max])\n",
        "\n",
        "    def _choose_random_action(self):\n",
        "        return np.random.choice(self.list_of_possible_actions)\n",
        "\n",
        "    def _create_new_state_entries_if_needed(self, state):\n",
        "        if (not isinstance(self.q_table.get(state), np.ndarray)):\n",
        "            self.q_table[state] = np.zeros(9)\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_state_key(board):\n",
        "     # Convert the Board configuration (Matrix) into a unique key for the state\n",
        "        return hashlib.sha1(board).hexdigest()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae72d45b",
      "metadata": {
        "id": "ae72d45b"
      },
      "source": [
        "We implemented a set of utility functions that will be used in the RL training procedure. Specifically:\n",
        "\n",
        "1. A `logger` function that prints the outcome of the game. \n",
        "2. A `choose_players` function that will be used to switch the first player to start to play at every episode.\n",
        "3. A `plot_rewards` function that plot the averaged rewards in a default window of $250$ games.    \n",
        "4. A `plot_metrics` function that plot the $M_{opt}$ (optimal metric) and the $M_{rand}$ (random metric)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8330bfed",
      "metadata": {
        "id": "8330bfed"
      },
      "outputs": [],
      "source": [
        "def logger(winner, player_1, player_2):\n",
        "    # Logs the information if specified\n",
        "    print('-------------------------------------------')\n",
        "    print(f'Game end, winner is player {str(winner)}')\n",
        "    print(f'Optimal player 1 = {str(player_1)}')\n",
        "    print(f'RL Agent player 2 = {str(player_2)}')\n",
        "    \n",
        "def choose_players(index):\n",
        "    if index%2 == 0:\n",
        "        player_1 = 'X'\n",
        "        player_2 = 'O'\n",
        "    else:\n",
        "        player_1 = 'O'\n",
        "        player_2 = 'X' \n",
        "        \n",
        "    return player_1,player_2     \n",
        "\n",
        "def choose_self_learning_player(step: int):\n",
        "    if step%2 == 0:\n",
        "        return \"X\"\n",
        "    else:\n",
        "        return \"O\"\n",
        "\n",
        "def plot_rewards(number_of_episodes, rewards, ax, label = \"\", plot_every=250):\n",
        "    episodes = np.arange(0,number_of_episodes)\n",
        "    bin_means, _, _ = binned_statistic(episodes, rewards, statistic=\"mean\", bins =(number_of_episodes)/plot_every, range=(0, number_of_episodes))\n",
        "    ax.plot(np.arange(0, number_of_episodes, plot_every), bin_means, label = label)\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Reward\")\n",
        "    ax.legend()\n",
        "    return\n",
        "\n",
        "def plot_loss(number_of_episodes, loss, ax, label = \"\", plot_every=250):\n",
        "    ax.plot(np.arange(0, number_of_episodes, plot_every), loss, label = label)\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.legend()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_metrics(number_of_episodes, metrics, ax, label):\n",
        "    ax.plot(np.arange(0, number_of_episodes, 250), metrics, label = label)\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Metric\")\n",
        "    ax.legend()\n",
        "    return\n",
        "\n",
        "def plot_epsilon(number_of_episodes, epsilon_list, ax, label):\n",
        "    ax.plot(range(number_of_episodes), epsilon_list, label = label)\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Epsilon\")\n",
        "    ax.legend()\n",
        "    return\n",
        "\n",
        "def convert_value_to_play_character(state: np.ndarray, q_values):\n",
        "    representation: List[List[str]] = [] \n",
        "    line: List[str] = [] \n",
        "    state = state.flatten()\n",
        "\n",
        "    for start in range(0, 9, 3):\n",
        "        line = []\n",
        "        for index in range(start, start+3):\n",
        "            if state[index] == 0:\n",
        "                line.append(f\"{round(float(q_values[index]) ,3)}\")\n",
        "            elif state[index] == 1:\n",
        "                line.append(\"X\")\n",
        "            elif state[index] == -1:\n",
        "                line.append(\"O\")\n",
        "        representation.append(line)\n",
        "    representation = np.array(representation)\n",
        "    return representation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5cc958b",
      "metadata": {
        "id": "d5cc958b"
      },
      "source": [
        "Now after implementing the needed functionalities of the RL agent we proceed to implementing a RL training procedure for Tic Tac Toe Player using Q-learning algorithm. With that, we will create a Q-table representing all the states and actions possible and we will progressively update the values in the table. The values in this table will depict the reward the agent expects to get if the corresponding action was chosen in the given state. In other words, if the values in the Q-table are the true values, then the optimal policy of the agent is to choose the actions whose corresponding Q-values are maximum in a given state. \n",
        "\n",
        "Since the Q-learning utilize current state, current action and next state Q-values, the Q-table is updated every 2 moves. With that, the Q-learning pipeline will look as follows:\n",
        "\n",
        "Given a certain current Tic-Tac-Toe state denoted by (S):\n",
        "\n",
        "1. RL agent choose an action (A) based on epsilon greedy and stores the chosen action. \n",
        "2. Optimal player choose an action based on the optimal level chosen.\n",
        "\n",
        "Now the RL agent observes a different state denoted by (S')\n",
        "\n",
        "3. RL agent choose an action (A') based on epsilon greedy and update the Q-table using the (S, A, S' and greedy action A*). A* might be different than A' as Q-learning is an-off policy strategy where A* is the action that maximize the Q-value for the given state (S').    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183541ba",
      "metadata": {
        "id": "183541ba"
      },
      "outputs": [],
      "source": [
        "def train_rl_agent_qlearning(environment: TictactoeEnv, number_of_episodes: int, optimal_level : float, epsilon:float, test_episode:int, verbose: bool = False):\n",
        "    # Initialize the Rewards and Test Metrics\n",
        "    rewards = np.zeros(number_of_episodes)\n",
        "    metrics_opt = np.zeros(int(number_of_episodes/test_episode))\n",
        "    metrics_rand = np.zeros(int(number_of_episodes/test_episode))\n",
        "\n",
        "    # Instantiate the Players\n",
        "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
        "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"O\")\n",
        "\n",
        "    for episode in tqdm(range(number_of_episodes)):\n",
        "        player_rl_agent.train()\n",
        "        \n",
        "        # Reset the Environment\n",
        "        environment.reset()\n",
        "        \n",
        "        #Observe the Environment\n",
        "        grid, _, _ = environment.observe()\n",
        "\n",
        "        #Choose the players X,O\n",
        "        optimal_player_character,rl_player_character = choose_players(index = episode)\n",
        "        player_optimal.player = optimal_player_character\n",
        "        player_rl_agent.player = rl_player_character\n",
        "        \n",
        "        # Give RL access to the board\n",
        "        player_rl_agent.observe_state(grid)\n",
        "\n",
        "        # Update nb of episode played\n",
        "        player_rl_agent.update_nb_of_episode_played(episode) \n",
        "\n",
        "\n",
        "        # store backup diagram \n",
        "        states_actions_rewards_observed: List[Tuple[np.ndarray, int, int]] = []\n",
        "        \n",
        "        for step in range(9):\n",
        "            if environment.current_player == player_optimal.player:\n",
        "                # choose an move\n",
        "                move = player_optimal.act(grid)\n",
        "\n",
        "                #Execute an action\n",
        "                grid, end, winner = environment.step(move, print_grid=False)\n",
        "            \n",
        "            else:\n",
        "                #RL current state and action\n",
        "                player_rl_agent.observe_state(grid)\n",
        "                rl_state = grid\n",
        "                rl_action = player_rl_agent.act(grid)\n",
        "\n",
        "                #Execute action\n",
        "                move = (int(rl_action/3),rl_action%3)\n",
        "                grid, end, winner = environment.step(move, print_grid=False)\n",
        "                rl_reward = environment.reward(rl_player_character)\n",
        "                states_actions_rewards_observed.append((rl_state,rl_action,rl_reward))\n",
        "\n",
        "                if len(states_actions_rewards_observed)==2:\n",
        "                    previous_state, previous_action, reward = states_actions_rewards_observed.pop(0)\n",
        "                    player_rl_agent.update_q_table(previous_state, previous_action,reward, rl_state)\n",
        "\n",
        "            if end:\n",
        "                # observe end reward\n",
        "                reward = environment.reward(rl_player_character)\n",
        "                rewards[episode] = reward\n",
        "                player_rl_agent.update_q_table(rl_state, rl_action, reward, grid, terminal_state=True )\n",
        "        \n",
        "                if verbose:\n",
        "                    logger(winner, optimal_player_character, rl_player_character)\n",
        "                    environment.render()\n",
        "                environment.reset()\n",
        "                break\n",
        "        \n",
        "        if (episode%test_episode == 0):\n",
        "            m_opt, m_rand = compute_metrics(environment, player_rl_agent, 500)\n",
        "            metrics_opt[int(episode/test_episode)] = m_opt\n",
        "            metrics_rand[int(episode/test_episode)] = m_rand\n",
        "    return player_rl_agent, rewards, metrics_opt, metrics_rand   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169da625",
      "metadata": {
        "id": "169da625"
      },
      "source": [
        "In order to assess the \"test\" performance of the RL agent, we will need to compute the M_opt and M_rand metric. The M_opt metric assesses how well the RL agent performs against the player following an optimal policy while the M_rand metric assesses how well the RL agent performs against a player playing a random policy.\n",
        "\n",
        "With that, two functions are needed. A function that allows the RL agent to play (without updating the Q_table) against the above described players and anoother function that computes the $M_{opt}$ and $M_{rand}$ using the equation:\n",
        "$ (N_{win} - N_{loss})/ N_{played} $ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "627a3cf5",
      "metadata": {
        "id": "627a3cf5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'RlAgent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-57b571bf3f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTictactoeEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_rl_agent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mRlAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_episodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplayer_rl_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnumber_of_rl_wins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_optimal_wins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_rl_agent_qlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_rl_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_of_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mM_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumber_of_rl_wins\u001b[0m \u001b[0;34m-\u001b[0m  \u001b[0mnumber_of_optimal_wins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_of_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RlAgent' is not defined"
          ]
        }
      ],
      "source": [
        "def compute_metrics(environment: TictactoeEnv, player_rl_agent:RlAgent, number_of_episodes:int, verbose: bool = False):\n",
        "    player_rl_agent.test()\n",
        "    \n",
        "    number_of_rl_wins, number_of_optimal_wins = evaluate_rl_agent_qlearning(environment, player_rl_agent,number_of_episodes, 0.0, verbose)\n",
        "    M_opt = (number_of_rl_wins -  number_of_optimal_wins)/number_of_episodes\n",
        "\n",
        "    number_of_rl_wins, number_of_optimal_wins = evaluate_rl_agent_qlearning(environment, player_rl_agent,number_of_episodes, 1.0, verbose)\n",
        "    M_rand = (number_of_rl_wins -  number_of_optimal_wins)/number_of_episodes\n",
        "\n",
        "    return M_opt, M_rand\n",
        "\n",
        "def evaluate_rl_agent_qlearning(environment: TictactoeEnv, player_rl_agent:RlAgent, number_of_episodes:int, optimal_level : float, verbose: bool = False):\n",
        "    # Instantiate the Players\n",
        "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
        "    player_rl_agent.player = \"O\"\n",
        "\n",
        "    number_of_rl_wins = 0\n",
        "    number_of_optimal_wins = 0\n",
        "    for episode in range(number_of_episodes):\n",
        "        # Reset the Environment\n",
        "        environment.reset()\n",
        "        \n",
        "        #Observe the Environment\n",
        "        grid, _, _ = environment.observe()\n",
        "\n",
        "        #Choose the players X,O\n",
        "        optimal_player_character, rl_player_character = choose_players(index = episode)\n",
        "        player_optimal.player = optimal_player_character\n",
        "        player_rl_agent.player = rl_player_character\n",
        "        \n",
        "        # Give RL access to the board\n",
        "        player_rl_agent.observe_state(grid)\n",
        "        \n",
        "        for step in range(9):\n",
        "            if environment.current_player == player_optimal.player:\n",
        "                move = player_optimal.act(grid)\n",
        "            else:\n",
        "                player_rl_agent.observe_state(grid)\n",
        "                rl_current_action = player_rl_agent.act(grid)\n",
        "                move = (int(rl_current_action/3),rl_current_action%3)\n",
        "        \n",
        "            grid, end, winner = environment.step(move, print_grid=False)\n",
        "            \n",
        "            \n",
        "            if end:\n",
        "                if winner == optimal_player_character:\n",
        "                    number_of_optimal_wins +=1\n",
        "                    \n",
        "                if winner == rl_player_character:\n",
        "                    number_of_rl_wins += 1\n",
        "                    \n",
        "                if verbose:\n",
        "                    logger(winner, optimal_player_character, rl_player_character)\n",
        "                    environment.render()\n",
        "                environment.reset()\n",
        "                break\n",
        "        \n",
        "\n",
        "    return number_of_rl_wins, number_of_optimal_wins   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607e5948",
      "metadata": {
        "id": "607e5948"
      },
      "source": [
        "### Learning from experts\n",
        "##### Question 1\n",
        "Finally, after implementing all the needed functions, we will now explore the effects of the epsilon on training. We will start by using a fixed epsilon through the training procedure (i.e accross all the episodes) and then we will compare the training preformance with that of a monotonically decreasing epsilon throughout the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58009e71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "58009e71",
        "outputId": "ac63e576-21e9-4eca-eb85-bc7e4237bbc8"
      },
      "outputs": [],
      "source": [
        "# Training with a fixed epsilon (Question 1)\n",
        "\n",
        "# RL Hyper-params\n",
        "number_of_episodes = 20000\n",
        "\n",
        "# Environment\n",
        "environment = TictactoeEnv()\n",
        "\n",
        "# plots \n",
        "fig,ax = plt.subplots(figsize=(8, 6))\n",
        "plt.title(\"Training with fixed epsilon\")\n",
        "\n",
        "# List of epsilons to try \n",
        "epsilons= [0.0, 0.1, 0.5, 0.9]\n",
        "\n",
        "for epsilon in epsilons:\n",
        "    # Train and get the rewards for a fixed epsilon \n",
        "    player_rl_agent, rewards, _, _ = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=0.5, epsilon=epsilon, test_episode=number_of_episodes)\n",
        "    plot_rewards(number_of_episodes, rewards, ax, label=f\"Fixed Epsilon: {epsilon}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db87915a-7248-4ff5-82e8-9b8ce18450e0",
      "metadata": {
        "id": "db87915a-7248-4ff5-82e8-9b8ce18450e0"
      },
      "source": [
        "#### Decreasing exploration\n",
        "##### Questions 2 & 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e75eeae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "7e75eeae",
        "outputId": "6347bca5-c5d2-47ef-8e78-9b34e5565bff"
      },
      "outputs": [],
      "source": [
        "# Training with a monotonically decreasing epsilon (Question 2 and Question 3)\n",
        "\n",
        "# RL Hyper-params\n",
        "number_of_episodes = 20000\n",
        "\n",
        "epsilon_min, epsilon_max = 0.1, 0.8\n",
        "\n",
        "# Environment\n",
        "environment = TictactoeEnv()\n",
        "\n",
        "# Plots\n",
        "fig1, axes1 = plt.subplots(1, 2, figsize=(16, 6))\n",
        "axes1[0].set_title(\"Epsilon\")\n",
        "axes1[1].set_title(\"Rewards\")\n",
        "fig2, axes2 = plt.subplots(1, 2, figsize=(16, 6))\n",
        "axes2[0].set_title(\"Optimal Metric\")\n",
        "axes2[1].set_title(\"Random Metric\")\n",
        "\n",
        "list_of_number_of_exploratory_games = [1, 5000, 15000, 30000, 40000]\n",
        " \n",
        "for number_of_exploratory_games in list_of_number_of_exploratory_games:\n",
        "    # Train and get the rewards, M_opt, M_rand for a decreasing monotinically epsilon \n",
        "    epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
        "    player_rl_agent, rewards, m_opt, m_rand = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=0.5, epsilon=epsilon, test_episode=250)\n",
        "    epsilon_list = [epsilon(n) for n in range(number_of_episodes)]\n",
        "    plot_epsilon(number_of_episodes, epsilon_list, axes1[0], label = f\"n* = {number_of_exploratory_games} \")\n",
        "    plot_rewards(number_of_episodes, rewards, axes1[1], label = f\"n* = {number_of_exploratory_games} \")\n",
        "    plot_metrics(number_of_episodes, m_opt, axes2[0], label = f\"n* = {number_of_exploratory_games} \")\n",
        "    plot_metrics(number_of_episodes, m_rand, axes2[1], label = f\"n* = {number_of_exploratory_games} \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca22352",
      "metadata": {
        "id": "8ca22352"
      },
      "source": [
        "#### Good experts and bad experts\n",
        "##### Question 4\n",
        "Now that we have explored the effect of the (fixed/varying) epsilon on the training performance, we will explore the effect of the optimality of the teacher on the training policy. Specifically, would the RL agent train better if it trained against an optimal policy or a random policy or somewhere in between?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb15418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "5fb15418",
        "outputId": "9daa7052-256a-44a2-c0f6-2aaff6f44309"
      },
      "outputs": [],
      "source": [
        "# Train with various Optimal level of the Agent for the optimal n* = 5000 (Question 4). \n",
        "\n",
        "# RL Hyper-params\n",
        "number_of_episodes = 20000\n",
        "epsilon_min,epsilon_max = 0.1, 0.8\n",
        "number_of_exploratory_games = 5000\n",
        "epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
        "\n",
        "# Environment\n",
        "environment = TictactoeEnv()\n",
        "\n",
        "# Plot\n",
        "fig,ax = plt.subplots(figsize=(8,6))\n",
        "plt.title(\"Rewards\")\n",
        "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
        "axes[0].set_title(\"Optimal Metric\")\n",
        "axes[1].set_title(\"Random Metric\")\n",
        "\n",
        "list_of_optimal_levels = [0,0.25,0.5,0.75,1]\n",
        "max_m_opts = {}\n",
        "max_m_rands = {}\n",
        "for optimal_level in list_of_optimal_levels:\n",
        "    # Train and get the rewards, M_opt, M_rand for a different optimality levels of the teacher  \n",
        "    player_rl_agent, rewards, m_opt, m_rand = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=optimal_level,epsilon=epsilon, test_episode=250)\n",
        "    plot_rewards(number_of_episodes, rewards, ax, label = f\"epsilon-Opt = {optimal_level} \")\n",
        "    plot_metrics(number_of_episodes, m_opt, axes[0], label = f\"epsilon-Opt = {optimal_level} \")\n",
        "    plot_metrics(number_of_episodes, m_rand, axes[1], label = f\"epsilon-Opt = {optimal_level} \")\n",
        "    max_m_opts[optimal_level] = np.max(m_opt)\n",
        "    max_m_rands[optimal_level] = np.max(m_rand)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c70ade-b0ba-4ac1-838f-c1144e69f3d9",
      "metadata": {
        "id": "08c70ade-b0ba-4ac1-838f-c1144e69f3d9"
      },
      "source": [
        "##### Question 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0e8836-72c3-48e0-bc51-670d0441851d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e0e8836-72c3-48e0-bc51-670d0441851d",
        "outputId": "7fae2de9-c49f-4389-d622-2cd0401c6eeb"
      },
      "outputs": [],
      "source": [
        "argmax_m_opt = max(max_m_opts, key=max_m_opts.get)\n",
        "max_m_opt = max_m_opts[argmax_m_opt]\n",
        "argmax_m_rand = max(max_m_rands, key=max_m_rands.get)\n",
        "max_m_rand = max_m_rands[argmax_m_rand]\n",
        "\n",
        "print(f'The highest value of Mopt that we could get achieve after {number_of_episodes} games is {max_m_opt}, for Îµ_opt = {argmax_m_opt}.')\n",
        "print(f'The highest value of Mrand that we could get achieve after {number_of_episodes} games is {max_m_rand}, for Îµ_opt = {argmax_m_rand}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3217deb1",
      "metadata": {
        "id": "3217deb1"
      },
      "source": [
        "### Learning by self-practice\n",
        "Now that we have deeply explored the performance the effect of the epsilon (exploration probability) and the optimality level of teacher. It would be interresting to see weather the RL Agent can learn by playing against itself since this capability is highly valued when we don't know what the optimal policy is (very common in real-life problems). \n",
        "\n",
        "With that, we implemented a new training procedure, where the RL agent plays againt it self by choosing actions from and updating the values of the same Q-table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a48c877",
      "metadata": {
        "id": "7a48c877"
      },
      "outputs": [],
      "source": [
        "def train_rl_agent_self_learning(environment: TictactoeEnv, number_of_episodes: int, epsilon:float, test_episode:int, verbose: bool = False):\n",
        "    # Initialize the Rewards and Test Metrics\n",
        "    metrics_opt = np.zeros(int(number_of_episodes/test_episode))\n",
        "    metrics_rand = np.zeros(int(number_of_episodes/test_episode))\n",
        "\n",
        "    # Instantiate the Players\n",
        "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"X\")\n",
        "\n",
        "    for episode in tqdm(range(number_of_episodes)):\n",
        "        player_rl_agent.train()\n",
        "        \n",
        "        # Reset the Environment\n",
        "        environment.reset()\n",
        "        \n",
        "        #Observe the Environment\n",
        "        grid, _, _ = environment.observe()\n",
        "\n",
        "        # Update nb of episode played\n",
        "        player_rl_agent.update_nb_of_episode_played(episode) \n",
        "\n",
        "        player_1_states_actions_rewards_observed: List[Tuple[np.ndarray, int, int]] = []\n",
        "        player_2_states_actions_rewards_observed: List[Tuple[np.ndarray, int, int]] = []\n",
        "        \n",
        "        for step in range(9):\n",
        "            # RL store player 1 current state and play player 1 current action \n",
        "            player_rl_agent.observe_state(grid, store_state=True)\n",
        "            rl_player_1_state = grid\n",
        "            rl_player_1_action = player_rl_agent.act(grid)\n",
        "\n",
        "            # execute current action\n",
        "            move = (int(rl_player_1_action/3),rl_player_1_action%3) \n",
        "            grid, end, _ = environment.step(move, print_grid=False)\n",
        "\n",
        "            # observe reward and store in the backup diagram\n",
        "            player_1_reward = environment.reward(\"X\")\n",
        "            player_1_states_actions_rewards_observed.append((rl_player_1_state, rl_player_1_action, player_1_reward))\n",
        "\n",
        "            if len(player_1_states_actions_rewards_observed)==2:\n",
        "                player_1_previous_state, player_1_previous_action, player_1_previous_reward = player_1_states_actions_rewards_observed.pop(0)\n",
        "                player_rl_agent.update_q_table(player_1_previous_state,player_1_previous_action,player_1_previous_reward,rl_player_1_state)\n",
        "\n",
        "            if end:           \n",
        "                player_rl_agent.update_q_table(rl_player_1_state, rl_player_1_action, player_1_reward, grid, terminal_state=True)\n",
        "                environment.reset()\n",
        "                break\n",
        "\n",
        "            # RL store player 2 current state and play player 2 current action \n",
        "            player_rl_agent.observe_state(grid, store_state=True)\n",
        "            rl_player_2_state = grid\n",
        "            rl_player_2_action = player_rl_agent.act(grid)\n",
        "\n",
        "            #execute current action\n",
        "            move = (int(rl_player_2_action/3),rl_player_2_action%3)\n",
        "            grid, end, _ = environment.step(move, print_grid=False)\n",
        "\n",
        "            # observe reward and store in the backup diagram\n",
        "            player_2_reward = environment.reward(\"O\")\n",
        "            player_2_states_actions_rewards_observed.append((rl_player_2_state, rl_player_2_action, player_2_reward))\n",
        "\n",
        "            if len(player_2_states_actions_rewards_observed)==2:\n",
        "                player_2_previous_state, player_2_previous_action, player_2_reward = player_2_states_actions_rewards_observed.pop(0)\n",
        "                player_rl_agent.update_q_table(player_2_previous_state,player_2_previous_action,player_2_reward, rl_player_2_state) \n",
        "          \n",
        "            if end:           \n",
        "                player_rl_agent.update_q_table(rl_player_2_state, rl_player_2_action,player_2_reward, grid, terminal_state=True)\n",
        "                environment.reset()\n",
        "                break            \n",
        "\n",
        "        \n",
        "        if (episode%test_episode == 0):\n",
        "            m_opt, m_rand = compute_metrics(environment, player_rl_agent, 500)\n",
        "            metrics_opt[int(episode/test_episode)] = m_opt\n",
        "            metrics_rand[int(episode/test_episode)] = m_rand\n",
        "    return player_rl_agent, metrics_opt, metrics_rand   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43e56f0",
      "metadata": {
        "id": "f43e56f0"
      },
      "source": [
        "##### Question 7\n",
        "With that, we will proceed as before by evaluating the effect of fixed/varying epsilon on the self-learning training procedure by allowing the RL agent play against it self and updating the same Q-table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f231b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "12f231b7",
        "outputId": "5a5d263b-1c30-4a57-a23b-96b4af34cbb7"
      },
      "outputs": [],
      "source": [
        "# Training by playing against it self with fixed epsilon (Question 7)\n",
        "\n",
        "# RL Hyper-params\n",
        "number_of_episodes = 20000\n",
        "\n",
        "# Environment\n",
        "environment = TictactoeEnv()\n",
        "\n",
        "# plots \n",
        "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
        "axes[0].set_title(\"Optimal Metric\")\n",
        "axes[1].set_title(\"Random Metric\")\n",
        "\n",
        "\n",
        "list_of_epsilons = [0.1,0.5,0.9]\n",
        "for epsilon in list_of_epsilons:\n",
        "# Train and get the M_opt, and M_rand for a fixed epsilon\n",
        "    player_rl_agent, m_opt, m_rand = train_rl_agent_self_learning(environment, number_of_episodes=number_of_episodes,epsilon=epsilon, test_episode=250)\n",
        "    plot_metrics(number_of_episodes,m_opt, axes[0],label=f\"Fixed Epsilon: {epsilon}\")\n",
        "    plot_metrics(number_of_episodes,m_rand, axes[1],label=f\"Fixed Epsilon: {epsilon}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557e992f-0567-476c-8a78-a84082dae74e",
      "metadata": {
        "id": "557e992f-0567-476c-8a78-a84082dae74e"
      },
      "source": [
        "##### Question 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c015f435",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "c015f435",
        "outputId": "5ddacb9e-5622-43e2-baeb-286af2471a9a"
      },
      "outputs": [],
      "source": [
        "# Training by playing against it self with decreasing epsilon (Question 8)\n",
        "\n",
        "# RL Hyper-params\n",
        "number_of_episodes = 20000\n",
        "epsilon_min,epsilon_max = 0.1, 0.8\n",
        "\n",
        "# Environment\n",
        "environment = TictactoeEnv()\n",
        "\n",
        "# plots \n",
        "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
        "axes[0].set_title(\"Optimal Metric\")\n",
        "axes[1].set_title(\"Random Metric\")\n",
        "\n",
        "list_of_number_of_exploratory_games = [1,5000,15000,30000,40000]\n",
        "\n",
        "# Define the best player agent \n",
        "max_m_opts = {}\n",
        "max_m_rands = {}\n",
        "players = {}\n",
        "for number_of_exploratory_games in list_of_number_of_exploratory_games:\n",
        "# Train and get the M_opt and M_rand for a number of exploratory games\n",
        "    epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
        "    player_rl_agent, m_opt, m_rand = train_rl_agent_self_learning(environment, number_of_episodes=number_of_episodes,epsilon=epsilon, test_episode=250)\n",
        "\n",
        "    plot_metrics(number_of_episodes,m_opt, axes[0],label= f\"n* = {number_of_exploratory_games} \")\n",
        "    plot_metrics(number_of_episodes,m_rand, axes[1],label= f\"n* = {number_of_exploratory_games} \")\n",
        "    max_m_opts[number_of_exploratory_games] = np.max(m_opt) \n",
        "    max_m_rands[number_of_exploratory_games] = np.max(m_rand)\n",
        "    players[number_of_exploratory_games] = player_rl_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee1e99cc-14b7-422a-8df9-3f9263cbd8c5",
      "metadata": {
        "id": "ee1e99cc-14b7-422a-8df9-3f9263cbd8c5"
      },
      "source": [
        "##### Question 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2035d69-358a-4982-be36-0822e127c83f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2035d69-358a-4982-be36-0822e127c83f",
        "outputId": "141c1f72-1ce5-4c0e-b998-1b64463ced02"
      },
      "outputs": [],
      "source": [
        "argmax_m_opt = max(max_m_opts, key=max_m_opts.get)\n",
        "max_m_opt = max_m_opts[argmax_m_opt]\n",
        "argmax_m_rand = max(max_m_rands, key=max_m_rands.get)\n",
        "max_m_rand = max_m_rands[argmax_m_rand]\n",
        "\n",
        "print(f'The highest value of Mopt that we could get achieve after {number_of_episodes} games is {max_m_opt}, for n* = {argmax_m_opt}.')\n",
        "print(f'The highest value of Mrand that we could get achieve after {number_of_episodes} games is {max_m_rand}, for n* = {argmax_m_rand}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf2afb9-6a0f-41e8-b44d-40ce3f2e2741",
      "metadata": {
        "id": "2cf2afb9-6a0f-41e8-b44d-40ce3f2e2741"
      },
      "source": [
        "##### Question 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb00b95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "ecb00b95",
        "outputId": "f0cc92ea-5afd-476f-b172-64f625f74926"
      },
      "outputs": [],
      "source": [
        "# get the best player agent\n",
        "player_rl_agent = players[argmax_m_opt]\n",
        "\n",
        "# Visualize the heat map\n",
        "number_of_visualized_states = 3\n",
        "fig, ax = plt.subplots(ncols=3, figsize=(16,6))\n",
        "\n",
        "for index in range(number_of_visualized_states):\n",
        "    state = player_rl_agent.observed_states[randrange(len(player_rl_agent.observed_states))]\n",
        "    state_key = player_rl_agent.get_state_key(state)\n",
        "    q_values = player_rl_agent.q_table[state_key]\n",
        "    q_values_reshaped = q_values.reshape((3,3))\n",
        "    sns.heatmap(q_values_reshaped, annot = convert_value_to_play_character(state, q_values), ax=ax[index], fmt=\"\")\n",
        "    ax[index].set_title(f\"Q-values of random state {index}\")\n",
        "    ax[index].set_xlabel(\"xpos\")\n",
        "    ax[index].set_ylabel(\"ypos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf170a65",
      "metadata": {
        "id": "bf170a65"
      },
      "source": [
        "## Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b05e9f30",
      "metadata": {
        "id": "b05e9f30"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import tensor\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2638d5eb",
      "metadata": {
        "id": "2638d5eb"
      },
      "outputs": [],
      "source": [
        "# A class that stores the current state, next_state, current_action\n",
        "class Transition:\n",
        "    def __init__(self, current_state: torch.Tensor, current_action: torch.Tensor, next_state:torch.Tensor, reward: torch.Tensor):\n",
        "        self.current_state = current_state\n",
        "        self.current_action = current_action\n",
        "        self.next_state = next_state\n",
        "        self.reward = reward\n",
        "        \n",
        "\n",
        "# A class that store the transitions     \n",
        "class ReplayBuffer:\n",
        "    def __init__(self,buffer_size)->None:\n",
        "        self.memory = deque([],maxlen=buffer_size)\n",
        "\n",
        "    def push(self, transition: Transition)->None:\n",
        "        self.memory.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size: int)->List[Transition]:\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def is_training_feasible(self, batch_size):\n",
        "        return len(self.memory)>batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "883d8c55",
      "metadata": {
        "id": "883d8c55"
      },
      "outputs": [],
      "source": [
        "## MODEL: Deep Q-Learning Model \n",
        "class DQN_model(nn.Module):\n",
        "    def __init__(self,dimension_of_state_values, number_of_actions):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dimension_of_state_values,128)\n",
        "        self.fc2 = nn.Linear(128,128)\n",
        "        self.fc3 = nn.Linear(128,number_of_actions)\n",
        "\n",
        "    def forward(self,val):\n",
        "        val = F.relu(self.fc1(val))\n",
        "        val = F.relu(self.fc2(val))\n",
        "        val = self.fc3(val)\n",
        "        return val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c975029f",
      "metadata": {
        "id": "c975029f"
      },
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "def get_training_data(training_transitions:List[Transition]):\n",
        "    training_states: List[torch.Tensor] = []\n",
        "    training_actions: List[torch.Tensor] = []\n",
        "    training_rewards: List[torch.Tensor] = []\n",
        "    training_next_states: List[torch.Tensor] = []\n",
        "    non_final_states_indices: List[int] = []\n",
        "\n",
        "    for index,transition in enumerate(training_transitions):\n",
        "        training_states.append(transition.current_state)\n",
        "        training_actions.append(tensor([transition.current_action]))\n",
        "        training_rewards.append(tensor([transition.reward]))\n",
        "        \n",
        "        # Final/Non-final next state\n",
        "        if transition.next_state is not None:\n",
        "            training_next_states.append(transition.next_state)\n",
        "            non_final_states_indices.append(index)\n",
        "    \n",
        "    training_states = torch.cat(training_states, dim=0)\n",
        "    training_actions = torch.cat(training_actions).reshape(len(training_actions),1)\n",
        "    training_rewards = torch.cat(training_rewards).reshape(len(training_rewards),1)\n",
        "    \n",
        "    if len(training_next_states) == 0:\n",
        "        next_states = tensor([])\n",
        "    else:    \n",
        "        next_states = torch.cat(training_next_states, dim = 0)\n",
        "\n",
        "\n",
        "    return training_states, training_actions, training_rewards, next_states, non_final_states_indices\n",
        "\n",
        "\n",
        "def get_rl_state(board:np.ndarray, rl_player):\n",
        "    flattened_board = board.flatten()\n",
        "    state_x = (flattened_board==1).astype(int)\n",
        "    state_o = (flattened_board==-1).astype(int)\n",
        "\n",
        "    # form the state\n",
        "    state = tensor([np.concatenate((state_x,state_o))]).float() if rl_player == 'X' else tensor([np.concatenate((state_o,state_x))]).float()\n",
        "    return state\n",
        "\n",
        "def check_if_feasible_move(current_state: torch.Tensor, action):\n",
        "    current_state_rl_value = current_state[:,action].item()\n",
        "    current_state_optimal_value = current_state[:,action+9].item()\n",
        "    return current_state_rl_value == current_state_optimal_value == 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "12746ff2",
      "metadata": {
        "id": "12746ff2"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d92da86dc9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtraining_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_transitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "###############TESTING#####################\n",
        "training_transitions = [Transition(current_state = tensor([[1,1,1]]), current_action = tensor([[0]]), reward=tensor([[1]]), next_state=tensor([[1,1,1]])),\n",
        "Transition(current_state = tensor([[2,2,2]]), current_action = tensor([[2]]), reward=tensor([[1]]), next_state=tensor([[1,1,1]])),\n",
        "Transition(current_state = tensor([[3,3,3]]), current_action = tensor([[1]]), reward=tensor([[1]]), next_state=tensor([[0.5,0.5,0.5]])),\n",
        "]\n",
        "\n",
        "training_states, actions, rewards, next_states = get_training_data(training_transitions)\n",
        "print(training_states)\n",
        "print(actions)\n",
        "print(rewards)\n",
        "print(next_states)\n",
        "\n",
        "print(training_states.gather(0,actions))\n",
        "training_states.max(1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f3448be0",
      "metadata": {
        "id": "f3448be0"
      },
      "outputs": [],
      "source": [
        "class DeepRLAgent:\n",
        "    def __init__(self, player: str, epsilon: float, discount_factor: float = 0.99, learning_rate:float = 5e-4)->None:\n",
        "        # Choose the Player (X,O)\n",
        "        self.player = player\n",
        "        \n",
        "        # DQN Hyperparams\n",
        "        self.epsilon = epsilon\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize the network model, target model\n",
        "        self.dqn_model:DQN_model = DQN_model(dimension_of_state_values=18, number_of_actions=9)\n",
        "        \n",
        "        self.target_model = DQN_model(dimension_of_state_values=18, number_of_actions=9)\n",
        "        self.target_model.load_state_dict(self.dqn_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # initialize the optimizer        \n",
        "        self.optimizer = optim.Adam(self.dqn_model.parameters(), lr = self.learning_rate)\n",
        "\n",
        "        # Test flag\n",
        "        self.is_test = False\n",
        "\n",
        "    def test(self):\n",
        "        # set to test behavior\n",
        "        self.is_test = True\n",
        "    \n",
        "    def train(self):\n",
        "        # set to train behavior\n",
        "        self.is_test = False\n",
        "\n",
        "    def act(self, state):\n",
        "        # act based on epsilon greedy strategy\n",
        "        if random.uniform(0,1) < self.epsilon:\n",
        "            return self._get_random_action()\n",
        "        else:\n",
        "            return self._get_best_action(state)\n",
        "            \n",
        "    def update_dqn_model(self, training_data:List[Transition]):\n",
        "        # update the model using Adam Optimizer\n",
        "        current_states, current_actions, current_rewards, non_terminal_next_states, non_terminal_indices = get_training_data(training_data)\n",
        "\n",
        "        # get the Q_values(s,a) \n",
        "        current_q_values: torch.Tensor = self.dqn_model(current_states).gather(1,current_actions)\n",
        "\n",
        "        # get next max_a Q_values(s,a*)\n",
        "        next_q_values: torch.Tensor = torch.zeros(current_actions.shape[0])\n",
        "        if len(non_terminal_next_states)>0:\n",
        "            next_q_values[non_terminal_indices]= self.target_model(non_terminal_next_states).max(1)[0].detach()\n",
        "        next_q_values = next_q_values.reshape(current_actions.shape[0],1)\n",
        "\n",
        "        # Error function \n",
        "        expected_q_values = current_rewards + self.discount_factor*next_q_values\n",
        "\n",
        "        # devise the error function\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss: torch.Tensor = criterion(current_q_values, expected_q_values)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        for param in self.dqn_model.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "        #print(\"loss: \", loss.item())\n",
        "        return loss\n",
        "    \n",
        "    def update_target_model(self):\n",
        "        # update the target network after C steps\n",
        "        self.target_model.load_state_dict(self.dqn_model.state_dict())\n",
        "\n",
        "\n",
        "    def _get_random_action(self):\n",
        "        # get random action\n",
        "        return random.randrange(0,9)\n",
        "\n",
        "    def _get_best_action(self, state: torch.Tensor):\n",
        "        # get best action\n",
        "        q_values:torch.Tensor = self.dqn_model(state)\n",
        "        best_action = q_values.max(1)[1]\n",
        "        return best_action.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "01f85858",
      "metadata": {
        "id": "01f85858"
      },
      "outputs": [],
      "source": [
        "def train_deep_learning_agent(environment: TictactoeEnv, number_of_episodes: int, optimal_level:float, epsilon:float, memory_buffer_size:int, batch_size:int = 64,target_model_update:int = 500, plot_every:int= 250, verbose: bool = False):\n",
        "    # Initialize the Rewards and Test Metrics\n",
        "    rewards = np.zeros(number_of_episodes)\n",
        "    loss_values: List[float] = []\n",
        "    average_loss_values: List[float] = []\n",
        "\n",
        "    # Instantiate the Players\n",
        "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
        "    player_rl_agent = DeepRLAgent(epsilon=epsilon, player=\"O\")\n",
        "\n",
        "    # create a memory buffer\n",
        "    memory = ReplayBuffer(memory_buffer_size)\n",
        "    \n",
        "    for episode in tqdm(range(number_of_episodes)):\n",
        "        player_rl_agent.train()\n",
        "        \n",
        "        # Reset the Environment\n",
        "        environment.reset()\n",
        "        \n",
        "        #Observe the Environment\n",
        "        grid, _, _ = environment.observe()\n",
        "\n",
        "        #Choose the players X,O\n",
        "        optimal_player_character,rl_player_character = choose_players(index = episode)\n",
        "        player_optimal.player = optimal_player_character\n",
        "        player_rl_agent.player = rl_player_character\n",
        "        \n",
        "        # store backup diagram \n",
        "        states_actions_rewards_observed: List[Tuple[torch.Tensor, int, int]] = []\n",
        "        \n",
        "        for step in range(9):\n",
        "            if environment.current_player == player_optimal.player:\n",
        "                move = player_optimal.act(grid)\n",
        "                grid, end, winner = environment.step(move, print_grid=False)\n",
        "            \n",
        "            else:\n",
        "                # Observe current state and execute desired action\n",
        "                rl_state = get_rl_state(grid, player_rl_agent.player)\n",
        "                rl_action = player_rl_agent.act(rl_state)\n",
        "                move = (int(rl_action/3),rl_action%3)\n",
        "\n",
        "                # if the RL Agent played an illegal move\n",
        "                if not check_if_feasible_move(rl_state, rl_action):\n",
        "                    rewards[episode] = -1\n",
        "                    memory.push(Transition(current_state=rl_state, current_action=rl_action, reward=-1, next_state=None))\n",
        "                    break\n",
        "                \n",
        "                # Step in the environment and observe the reward\n",
        "                grid, end, winner = environment.step(move, print_grid=False)\n",
        "                reward = environment.reward(rl_player_character)\n",
        "\n",
        "                states_actions_rewards_observed.append((rl_state, rl_action, reward))\n",
        "                if len(states_actions_rewards_observed) == 2:\n",
        "                    # Store the interaction in a replay buffer\n",
        "                    previous_state, previous_action, previous_reward = states_actions_rewards_observed.pop(0)\n",
        "                    memory.push(Transition(current_state=previous_state, current_action=previous_action, reward=previous_reward, next_state=rl_state))\n",
        "\n",
        "                if memory.is_training_feasible(batch_size):\n",
        "                    training_data = memory.sample(batch_size)\n",
        "                    loss: torch.Tensor = player_rl_agent.update_dqn_model(training_data)\n",
        "                    loss_values.append(loss.item())\n",
        "            if end:\n",
        "                reward = environment.reward(rl_player_character)\n",
        "                rewards[episode] = reward\n",
        "                \n",
        "                memory.push(Transition(current_state=rl_state, current_action=rl_action, reward=reward, next_state=None))\n",
        "       \n",
        "                if verbose:\n",
        "                    logger(winner, optimal_player_character, rl_player_character)\n",
        "                    environment.render()\n",
        "                environment.reset()\n",
        "                break\n",
        "        \n",
        "        if ((episode+1)%target_model_update) == 0:\n",
        "            player_rl_agent.update_target_model()\n",
        "        \n",
        "        if ((episode+1)%plot_every) == 0:\n",
        "            average_loss_values.append(np.mean(np.array(loss_values)))\n",
        "            loss_values = []     \n",
        "\n",
        "    return player_rl_agent, rewards, average_loss_values \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "01e57293",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 500/500 [00:03<00:00, 150.75it/s]\n",
            "100%|ââââââââââ| 500/500 [00:04<00:00, 124.69it/s]\n",
            "100%|ââââââââââ| 500/500 [00:03<00:00, 128.33it/s]\n",
            "100%|ââââââââââ| 500/500 [00:03<00:00, 158.90it/s]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training with a fixed epsilon (Question 11)\n",
        "\n",
        "# Environment\n",
        "environment = TictactoeEnv()\n",
        "# RL Hyper params\n",
        "number_of_episodes=500\n",
        "epsilon = 0.1\n",
        "memory_buffer_size = 10000\n",
        "optimal_level = 0.5\n",
        "\n",
        "# plots \n",
        "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
        "plt.title(\"Training with fixed epsilon\")\n",
        "\n",
        "# List of epsilons to try \n",
        "epsilons= [0.0, 0.1, 0.5, 0.9]\n",
        "\n",
        "for epsilon in epsilons:\n",
        "    # Train and get the rewards for a fixed epsilon \n",
        "    player_rl_agent, rewards, loss = train_deep_learning_agent(environment, number_of_episodes, optimal_level, epsilon, memory_buffer_size)\n",
        "    plot_rewards(number_of_episodes, rewards, axes[0], label=f\"Fixed Epsilon: {epsilon}\")\n",
        "    plot_loss(number_of_episodes, loss, axes[1], label=f\"Fixed Epsilon: {epsilon}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "322190_325932.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
