{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ebbf09-059d-46da-b87b-25516ca17316",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (CS-456)\n",
    "## Miniproject 1: Tic Tac Toe\n",
    "- MickaÃ«l Achkar (322190)\n",
    "- Yehya El Hassan (325932)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f5258-99aa-4582-9c6c-28f49e65e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from typing import Dict, List\n",
    "import hashlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3025cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RlAgent():\n",
    "    def __init__(self,player:str,epsilon:float,learning_rate:float = 0.05,discount_factor:float = 0.99):\n",
    "        # Choose the Player (X,O)\n",
    "        self.player = player\n",
    "\n",
    "        # Choose the exploration/exploitation factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "        # RL training hyper params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Form the the q_table and initialize it to empty\n",
    "        self.q_table:Dict[str,np.ndarray] = {}\n",
    "\n",
    "        # Actions take values between 0 and 9 representing the possible positions on the board\n",
    "        self.list_of_possible_actions:List[int] = [] \n",
    "\n",
    "        # Initialize the current reward\n",
    "        self.reward = 0\n",
    "\n",
    "    def initialize_board(self,board: np.ndarray):\n",
    "        # Initailize the board\n",
    "        self._update_board(board)\n",
    "        self._update_list_of_possible_actions()\n",
    "        \n",
    "    def environment_info(self, board, reward):\n",
    "        # Recieves information from the environment\n",
    "        self._update_board(board)\n",
    "        self._update_reward(reward)\n",
    "        self._update_list_of_possible_actions()\n",
    "\n",
    "    def act(self, board: np.ndarray):\n",
    "        # Sample from a uniform distribution\n",
    "        current_state = get_state_from_board(board)\n",
    "        if (epsilon>random.uniform(0,1)):\n",
    "            return self._choose_random_action()\n",
    "        else:\n",
    "            return self._choose_best_action(current_state)\n",
    "\n",
    "    def update_q_table(self,current_state, current_action, next_state, next_action):\n",
    "        # update the q_table\n",
    "        current_state = get_state_from_board(current_state)\n",
    "        next_state = get_state_from_board(next_state)\n",
    "\n",
    "        # create new entries if needed\n",
    "        self._create_new_state_entries_if_needed(current_state)\n",
    "        self._create_new_state_entries_if_needed(next_state)\n",
    "        \n",
    "        self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(self.reward + self.discount_factor*(self.q_table[next_state][next_action]) - self.q_table[current_state][current_action])\n",
    "\n",
    "\n",
    "    def _update_board(self,board):\n",
    "        # Get's the latest board configuration from the Game\n",
    "        self.board = board\n",
    "    \n",
    "    def _update_list_of_possible_actions(self):\n",
    "        # Get's the available positions on the board\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if self.board[pos] == 0:\n",
    "                avail.append(i)\n",
    "        self.list_of_possible_actions = avail\n",
    "        return self.list_of_possible_actions\n",
    "\n",
    "    def _update_reward(self, reward):\n",
    "        # Updates the current reward\n",
    "        self.reward = reward\n",
    "\n",
    "    def _choose_best_action(self, current_state):\n",
    "        self._create_new_state_entries_if_needed(current_state)\n",
    "        maximum_q_values = np.where(self.q_table[current_state][self.list_of_possible_actions] == np.max(self.q_table[current_state][self.list_of_possible_actions]))[0]\n",
    "        random_between_max = np.random.choice(maximum_q_values)\n",
    "        return (self.list_of_possible_actions[random_between_max])\n",
    "\n",
    "    def _choose_random_action(self):\n",
    "        return np.random.choice(self.list_of_possible_actions)\n",
    "\n",
    "    def _create_new_state_entries_if_needed(self, state):\n",
    "        if (not isinstance(self.q_table.get(state), np.ndarray)):\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "\n",
    "\n",
    "\n",
    "def get_state_from_board(board):\n",
    "     # Convert the Board configuration (Matrix) into a unique key for the state\n",
    "    return hashlib.sha1(board).hexdigest()\n",
    "\n",
    "def logger(winner, player_1, player_2):\n",
    "    # Logs the information if specified\n",
    "    print('-------------------------------------------')\n",
    "    print(f'Game end, winner is player {str(winner)}')\n",
    "    print(f'Optimal player 1 = {str(player_1)}')\n",
    "    print(f'RL Agent player 2 = {str(player_2)}')\n",
    "    \n",
    "def choose_players(index):\n",
    "    if index%2 == 0:\n",
    "        player_1 = 'X'\n",
    "        player_2 = 'O'\n",
    "    else:\n",
    "        player_1 = 'O'\n",
    "        player_2 = 'X' \n",
    "        \n",
    "    return player_1,player_2   \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc958b",
   "metadata": {},
   "source": [
    "Implementing a Tic Tac Toc Player using Q-learning. With that, we will create a Q-table representing all the states and actions possible and we will progressively update the values in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent(environment: TictactoeEnv,number_of_episodes: int, optimal_level : float, epsilon:float, verbose: bool = False):\n",
    "    # Initialize the number of RL wins\n",
    "    number_of_rl_wins = 0\n",
    "\n",
    "    # Initialize the Rewards\n",
    "    num_of_reward = int(number_of_episodes/50)\n",
    "    rewards = np.zeros(num_of_reward)\n",
    "    reward_index = 0\n",
    "    reward = 0\n",
    "\n",
    "    # Instantiate the Players\n",
    "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
    "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"O\")\n",
    "\n",
    "    \n",
    "    for index in tqdm(range(number_of_episodes)):\n",
    "        \n",
    "        # Reset the Environment\n",
    "        environment.reset()\n",
    "        \n",
    "        #Observe the Environment\n",
    "        grid, _, _ = environment.observe()\n",
    "\n",
    "        #Choose the players X,O\n",
    "        player_1,player_2 = choose_players(index = index)\n",
    "        player_optimal.player = player_1\n",
    "        player_rl_agent.player = player_2\n",
    "        \n",
    "        # Give RL access to the board\n",
    "        player_rl_agent.initialize_board(grid)\n",
    "\n",
    "        # Number of RL movements \n",
    "        number_of_rl_movements = 0\n",
    "        \n",
    "        for step in range(9):\n",
    "            if environment.current_player == player_optimal.player:\n",
    "                move = player_optimal.act(grid)\n",
    "            else:\n",
    "                if (number_of_rl_movements%2 ==0):\n",
    "                    rl_current_action = player_rl_agent.act(grid)\n",
    "                    rl_current_state = grid\n",
    "                    move = (int(rl_current_action/3),rl_current_action%3)\n",
    "                else:\n",
    "                    rl_next_action = player_rl_agent.act(grid)\n",
    "                    rl_next_state = grid\n",
    "                    move = (int(rl_next_action/3),rl_next_action%3)\n",
    "                    player_rl_agent.update_q_table(rl_current_state, rl_current_action, rl_next_state, rl_next_action)\n",
    "\n",
    "                number_of_rl_movements+=1\n",
    "\n",
    "        \n",
    "            grid, end, winner = environment.step(move, print_grid=False)\n",
    "            player_rl_agent.environment_info(grid,environment.reward(player_2))\n",
    "            \n",
    "            if end:\n",
    "                reward += environment.reward(player_2)\n",
    "                #Update with the current state action as the next state as current state TODO: check\n",
    "                if (number_of_rl_movements%2 ==0):\n",
    "                    player_rl_agent.update_q_table(rl_current_state, rl_current_action, rl_current_state, rl_current_action)\n",
    "        \n",
    "                if winner == player_rl_agent.player:\n",
    "                    number_of_rl_wins+=1\n",
    "                if verbose:\n",
    "                    logger(winner, player_1, player_2)\n",
    "                    environment.render()\n",
    "                environment.reset()\n",
    "                break\n",
    "        \n",
    "        if (index%50 == 0):\n",
    "            rewards[reward_index] = reward\n",
    "            reward = 0\n",
    "            reward_index+=1\n",
    "\n",
    "    if reward_index<(num_of_reward)-1:\n",
    "        rewards[(num_of_reward)-1] = reward\n",
    "\n",
    "    return player_rl_agent, rewards   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58009e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Hyper-params\n",
    "number_of_episodes = 100000\n",
    "epsilon = 0.1\n",
    "\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "player_rl_agent, rewards = train_rl_agent(environment, number_of_episodes=number_of_episodes, optimal_level=0.5,epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
