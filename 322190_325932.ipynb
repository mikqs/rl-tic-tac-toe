{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ebbf09-059d-46da-b87b-25516ca17316",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (CS-456)\n",
    "## Miniproject 1: Tic Tac Toe\n",
    "- MickaÃ«l Achkar (322190)\n",
    "- Yehya El Hassan (325932)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f5258-99aa-4582-9c6c-28f49e65e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from typing import Dict, List, Union, Callable\n",
    "import hashlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3025cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RlAgent():\n",
    "    def __init__(self,player:str,epsilon:Union[float, Callable],learning_rate:float = 0.05,discount_factor:float = 0.99):\n",
    "        # Choose the Player (X,O)\n",
    "        self.player = player\n",
    "\n",
    "        # Choose the exploration/exploitation factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # RL training hyper params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Form the the q_table and initialize it to empty\n",
    "        self.q_table:Dict[str,np.ndarray] = {}\n",
    "\n",
    "        # Actions take values between 0 and 9 representing the possible positions on the board\n",
    "        self.list_of_possible_actions:List[int] = [] \n",
    "\n",
    "        # Initialize the current reward\n",
    "        self.reward = 0\n",
    "\n",
    "        # Update current episode number\n",
    "        self.current_episode = 0\n",
    "        \n",
    "    \n",
    "    def observe_state(self, board):\n",
    "        # Observe the current state of the environment\n",
    "        self._update_board(board)\n",
    "        self._update_list_of_possible_actions()\n",
    "\n",
    "    def observe_reward(self,reward):\n",
    "        # Observe the current reward from the environment\n",
    "        self._update_reward(reward)\n",
    "\n",
    "    def update_nb_of_episode_played(self, episode_number):\n",
    "        self.current_episode = episode_number\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        # Retun the current epsilob \n",
    "        if (isinstance(self.epsilon, Callable)):\n",
    "            return self.epsilon(self.current_episode)\n",
    "        else:\n",
    "            return self.epsilon\n",
    "\n",
    "    def act(self, board: np.ndarray):\n",
    "        # Sample from a uniform distribution\n",
    "        current_state = self.get_state_key(board)\n",
    "\n",
    "        if (random.uniform(0,1)<self.get_epsilon()):\n",
    "            return self._choose_random_action()\n",
    "        else:\n",
    "            return self._choose_best_action(current_state)\n",
    "\n",
    "    def update_q_table(self,current_board_config, current_action, next_board_config, terminal_state = False):\n",
    "        # update the q_table\n",
    "        current_state = self.get_state_key(current_board_config)\n",
    "        next_state = self.get_state_key(next_board_config)\n",
    "\n",
    "        # create new entries if needed\n",
    "        self._create_new_state_entries_if_needed(current_state)\n",
    "        self._create_new_state_entries_if_needed(next_state)\n",
    "\n",
    "        if not terminal_state:\n",
    "            best_action = self._choose_best_action(next_state)\n",
    "            self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(self.reward + self.discount_factor*(self.q_table[next_state][best_action]) - self.q_table[current_state][current_action])\n",
    "        else:\n",
    "            self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(self.reward - self.q_table[current_state][current_action])\n",
    "\n",
    "    def _update_board(self,board):\n",
    "        # Get's the latest board configuration from the Game\n",
    "        self.board = board\n",
    "    \n",
    "    def _update_list_of_possible_actions(self):\n",
    "        # Get's the available positions on the board\n",
    "        available_actions = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if self.board[pos] == 0:\n",
    "                available_actions.append(i)\n",
    "        self.list_of_possible_actions = available_actions\n",
    "        return self.list_of_possible_actions\n",
    "\n",
    "    def _update_reward(self, reward):\n",
    "        # Updates the current reward\n",
    "        self.reward = reward\n",
    "\n",
    "    def _choose_best_action(self, current_state):\n",
    "        self._create_new_state_entries_if_needed(current_state)\n",
    "        maximum_q_values_idx = np.where(self.q_table[current_state][self.list_of_possible_actions] == np.max(self.q_table[current_state][self.list_of_possible_actions]))[0]\n",
    "        random_max = np.random.choice(maximum_q_values_idx)\n",
    "        return (self.list_of_possible_actions[random_max])\n",
    "\n",
    "    def _choose_random_action(self):\n",
    "        return np.random.choice(self.list_of_possible_actions)\n",
    "\n",
    "    def _create_new_state_entries_if_needed(self, state):\n",
    "        if (not isinstance(self.q_table.get(state), np.ndarray)):\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_state_key(board):\n",
    "     # Convert the Board configuration (Matrix) into a unique key for the state\n",
    "        return hashlib.sha1(board).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logger(winner, player_1, player_2):\n",
    "    # Logs the information if specified\n",
    "    print('-------------------------------------------')\n",
    "    print(f'Game end, winner is player {str(winner)}')\n",
    "    print(f'Optimal player 1 = {str(player_1)}')\n",
    "    print(f'RL Agent player 2 = {str(player_2)}')\n",
    "    \n",
    "def choose_players(index):\n",
    "    if index%2 == 0:\n",
    "        player_1 = 'X'\n",
    "        player_2 = 'O'\n",
    "    else:\n",
    "        player_1 = 'O'\n",
    "        player_2 = 'X' \n",
    "        \n",
    "    return player_1,player_2   \n",
    "\n",
    "def plot_rewards(number_of_episodes,rewards):\n",
    "    episodes = np.arange(0,number_of_episodes)\n",
    "    bin_means,_,_ = binned_statistic(episodes ,rewards, statistic=\"mean\", bins =(number_of_episodes)/250, range=(0,number_of_episodes))\n",
    "    plt.plot(np.arange(0,number_of_episodes,250),bin_means)\n",
    "    return\n",
    "\n",
    "def plot_metrics(number_of_episodes,metrics, title):\n",
    "    plt.plot(np.arange(0,number_of_episodes,250),metrics)\n",
    "    plt.title(title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc958b",
   "metadata": {},
   "source": [
    "Implementing a Tic Tac Toc Player using Q-learning. With that, we will create a Q-table representing all the states and actions possible and we will progressively update the values in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183541ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent_qlearning(environment: TictactoeEnv, number_of_episodes: int, optimal_level : float, epsilon:float, test_episode:int, verbose: bool = False):\n",
    "    # Initialize the Rewards and Test Metrics\n",
    "    rewards = np.zeros(number_of_episodes)\n",
    "    metrics_opt = np.zeros(int(number_of_episodes/test_episode))\n",
    "    metrics_rand = np.zeros(int(number_of_episodes/test_episode))\n",
    "\n",
    "    # Instantiate the Players\n",
    "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
    "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"O\")\n",
    "\n",
    "    for episode in tqdm(range(number_of_episodes)):\n",
    "        \n",
    "        # Reset the Environment\n",
    "        environment.reset()\n",
    "        \n",
    "        #Observe the Environment\n",
    "        grid, _, _ = environment.observe()\n",
    "\n",
    "        #Choose the players X,O\n",
    "        optimal_player_character,rl_player_character = choose_players(index = episode)\n",
    "        player_optimal.player = optimal_player_character\n",
    "        player_rl_agent.player = rl_player_character\n",
    "        \n",
    "        # Give RL access to the board\n",
    "        player_rl_agent.observe_state(grid)\n",
    "\n",
    "        # Update nb of episode played\n",
    "        player_rl_agent.update_nb_of_episode_played(episode) \n",
    "\n",
    "        # Number of RL movements \n",
    "        number_of_rl_movements = 0\n",
    "        \n",
    "        for step in range(9):\n",
    "            if environment.current_player == player_optimal.player:\n",
    "                move = player_optimal.act(grid)\n",
    "            else:\n",
    "                if (number_of_rl_movements%2 ==0):\n",
    "                    rl_current_state = grid\n",
    "                    rl_current_action = player_rl_agent.act(grid)\n",
    "                    move = (int(rl_current_action/3),rl_current_action%3)\n",
    "                else:\n",
    "                    rl_next_state = grid\n",
    "                    rl_next_action = player_rl_agent.act(grid)\n",
    "                    move = (int(rl_next_action/3),rl_next_action%3)\n",
    "                    player_rl_agent.update_q_table(rl_current_state, rl_current_action, rl_next_state)\n",
    "\n",
    "                number_of_rl_movements+=1\n",
    "\n",
    "        \n",
    "            grid, end, winner = environment.step(move, print_grid=False)\n",
    "            player_rl_agent.observe_state(grid)\n",
    "            player_rl_agent.observe_reward(environment.reward(rl_player_character))\n",
    "            \n",
    "            if end:\n",
    "                rewards[episode] = (environment.reward(rl_player_character))\n",
    "                #Update with the current state action as the next state as current state TODO: check\n",
    "                \n",
    "                player_rl_agent.update_q_table(rl_current_state, rl_current_action, grid, terminal_state=True )\n",
    "        \n",
    "                if verbose:\n",
    "                    logger(winner, optimal_player_character, rl_player_character)\n",
    "                    environment.render()\n",
    "                environment.reset()\n",
    "                break\n",
    "        \n",
    "        if (episode%test_episode == 0):\n",
    "            m_opt, m_rand = compute_metrics(environment, player_rl_agent, 500)\n",
    "            metrics_opt[int(episode/test_episode)] = m_opt\n",
    "            metrics_rand[int(episode/test_episode)] = m_rand\n",
    "    return player_rl_agent, rewards, metrics_opt, metrics_rand   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(environment: TictactoeEnv, player_rl_agent:RlAgent, number_of_episodes:int, verbose: bool = False):\n",
    "    number_of_rl_wins, number_of_optimal_wins = evaluate_rl_agent_qlearning(environment, player_rl_agent,number_of_episodes, 0.0, verbose)\n",
    "    M_opt = (number_of_rl_wins -  number_of_optimal_wins)/number_of_episodes\n",
    "\n",
    "    number_of_rl_wins, number_of_optimal_wins = evaluate_rl_agent_qlearning(environment, player_rl_agent,number_of_episodes, 1.0, verbose)\n",
    "    M_rand = (number_of_rl_wins -  number_of_optimal_wins)/number_of_episodes\n",
    "\n",
    "    return M_opt, M_rand\n",
    "\n",
    "def evaluate_rl_agent_qlearning(environment: TictactoeEnv, player_rl_agent:RlAgent, number_of_episodes:int, optimal_level : float, verbose: bool = False):\n",
    "    # Instantiate the Players\n",
    "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
    "    player_rl_agent.player = \"O\"\n",
    "\n",
    "    number_of_rl_wins = 0\n",
    "    number_of_optimal_wins = 0\n",
    "    for episode in range(number_of_episodes):\n",
    "        # Reset the Environment\n",
    "        environment.reset()\n",
    "        \n",
    "        #Observe the Environment\n",
    "        grid, _, _ = environment.observe()\n",
    "\n",
    "        #Choose the players X,O\n",
    "        optimal_player_character,rl_player_character = choose_players(index = episode)\n",
    "        player_optimal.player = optimal_player_character\n",
    "        player_rl_agent.player = rl_player_character\n",
    "        \n",
    "        # Give RL access to the board\n",
    "        player_rl_agent.observe_state(grid)\n",
    "        \n",
    "        for step in range(9):\n",
    "            if environment.current_player == player_optimal.player:\n",
    "                move = player_optimal.act(grid)\n",
    "            else:\n",
    "                rl_current_action = player_rl_agent.act(grid)\n",
    "                move = (int(rl_current_action/3),rl_current_action%3)\n",
    "        \n",
    "            grid, end, winner = environment.step(move, print_grid=False)\n",
    "            player_rl_agent.observe_state(grid)\n",
    "            \n",
    "            \n",
    "            if end:\n",
    "                if winner == optimal_player_character:\n",
    "                    number_of_optimal_wins +=1\n",
    "                    \n",
    "                if winner == rl_player_character:\n",
    "                    number_of_rl_wins += 1\n",
    "                    \n",
    "                if verbose:\n",
    "                    logger(winner, optimal_player_character, rl_player_character)\n",
    "                    environment.render()\n",
    "                environment.reset()\n",
    "                break\n",
    "        \n",
    "\n",
    "    return number_of_rl_wins, number_of_optimal_wins   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e5948",
   "metadata": {},
   "source": [
    "Train using Q_learning and fixed epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58009e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "epsilon = 0.1\n",
    "\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "player_rl_agent, rewards, m_opt, m_rand = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=0.5,epsilon=epsilon, test_episode=250)\n",
    "\n",
    "plot_rewards(number_of_episodes,rewards)\n",
    "plot_metrics(number_of_episodes,m_opt,  \"Optimal Metric Evolution over time.\")\n",
    "plot_metrics(number_of_episodes,m_rand, \"Random Metric Evolution over time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be4695",
   "metadata": {},
   "source": [
    "Train using Q_learning and variable epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "epsilon_min,epsilon_max = 0.1, 0.8\n",
    "number_of_exploratory_games = 10000\n",
    "epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
    "\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "player_rl_agent, rewards, m_opt, m_rand = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=0.5,epsilon=epsilon, test_episode=250)\n",
    "\n",
    "plot_rewards(number_of_episodes,rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
