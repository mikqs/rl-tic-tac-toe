{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ebbf09-059d-46da-b87b-25516ca17316",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (CS-456)\n",
    "## Miniproject 1: Tic Tac Toe\n",
    "- MickaÃ«l Achkar (322190)\n",
    "- Yehya El Hassan (325932)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f5258-99aa-4582-9c6c-28f49e65e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from typing import Dict, List, Union, Callable\n",
    "import hashlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binned_statistic\n",
    "from random import randrange\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0fcc44",
   "metadata": {},
   "source": [
    "In order to implement an agent that utilizes reinforecement learning to play Tic Tac Toe, we implemented an RLAgent class. This class handles all the needed functionalities of the RL agent such as:\n",
    "\n",
    "* `observe_state` which stores the game board and updates the list of possible actions.\n",
    "* `get_epsilon` which returns the value of epsilon. Epsilon can be either fixed or monotonically decreasing as discussed later.\n",
    "* `update_nb_of_episode_played` which updates the number of episodes played to calculate the value of the monotonically decreasing epsilon.\n",
    "* `observe_reward` which updates the internal stored value of the reward given by the environment.\n",
    "* `act` which selects an action from a list of possibled actions based on $\\epsilon$-greedy strategy.\n",
    "* `update_q_table` which stores and updates a table that contains a $Q$-value for every state and action possible in the given environment.\n",
    "\n",
    "In addition, other internal functions are denoted by a `_` prefix to implement the above discussed functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3025cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RlAgent():\n",
    "    def __init__(self, player: str, epsilon: Union[float, Callable], learning_rate: float = 0.05, discount_factor: float = 0.99):\n",
    "        # Choose the Player (X,O)\n",
    "        self.player = player\n",
    "\n",
    "        # Choose the exploration/exploitation factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # RL training hyper params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Form the the q_table and initialize it to empty\n",
    "        self.q_table: Dict[str,np.ndarray] = {}\n",
    "\n",
    "        # Store all observed states for heat map generation\n",
    "        self.observed_states: List[np.ndarray] = []\n",
    "\n",
    "        # Actions take values between 0 and 9 representing the possible positions on the board\n",
    "        self.list_of_possible_actions: List[int] = [] \n",
    "\n",
    "        # Initialize the current reward\n",
    "        self.reward = 0\n",
    "\n",
    "        # Update current episode number\n",
    "        self.current_episode = 0\n",
    "\n",
    "        # Update the is_test flag\n",
    "        self.is_test = False\n",
    "    \n",
    "    def test(self):\n",
    "        # set to testing behavior\n",
    "        self.is_test = True\n",
    "    \n",
    "    def train(self):\n",
    "        # set to training behavior\n",
    "        self.is_test = False\n",
    "\n",
    "    def observe_state(self, board, store_state:bool = False):\n",
    "        # Observe the current state of the environment\n",
    "        if store_state:\n",
    "            self.observed_states.append(board)\n",
    "        self._update_board(board)\n",
    "        self._update_list_of_possible_actions()\n",
    "\n",
    "    def observe_reward(self, reward):\n",
    "        # Observe the current reward from the environment\n",
    "        self._update_reward(reward)\n",
    "\n",
    "    def update_nb_of_episode_played(self, episode_number):\n",
    "        self.current_episode = episode_number\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        # Retun the current epsilon\n",
    "        if (isinstance(self.epsilon, Callable)):\n",
    "            return self.epsilon(self.current_episode)\n",
    "        else:\n",
    "            return self.epsilon\n",
    "\n",
    "    def act(self, board: np.ndarray):\n",
    "        # TODO: check if passing the board is needed.\n",
    "        current_state = self.get_state_key(board)\n",
    "\n",
    "        if (not self.is_test):\n",
    "            # Sample from a uniform distribution\n",
    "            if (random.uniform(0,1)<self.get_epsilon()):\n",
    "                return self._choose_random_action()\n",
    "            else:\n",
    "                return self._choose_best_action(current_state)\n",
    "        else:\n",
    "            return self._choose_best_action(current_state)\n",
    "\n",
    "    def update_q_table(self, current_board_config, current_action, next_board_config, terminal_state = False):\n",
    "        # update the q_table\n",
    "        current_state = self.get_state_key(current_board_config)\n",
    "        next_state = self.get_state_key(next_board_config)\n",
    "\n",
    "        # create new entries if needed\n",
    "        self._create_new_state_entries_if_needed(current_state)\n",
    "        self._create_new_state_entries_if_needed(next_state)\n",
    "\n",
    "        if not terminal_state:\n",
    "            best_action = self._choose_best_action(next_state)\n",
    "            self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(self.reward + self.discount_factor*(self.q_table[next_state][best_action]) - self.q_table[current_state][current_action])\n",
    "        else:\n",
    "            self.q_table[current_state][current_action] = self.q_table[current_state][current_action] + self.learning_rate*(self.reward - self.q_table[current_state][current_action])\n",
    "\n",
    "    def _update_board(self, board):\n",
    "        # Get's the latest board configuration from the Game\n",
    "        self.board = board\n",
    "    \n",
    "    def _update_list_of_possible_actions(self):\n",
    "        # Get's the available positions on the board\n",
    "        available_actions = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if self.board[pos] == 0:\n",
    "                available_actions.append(i)\n",
    "        self.list_of_possible_actions = available_actions\n",
    "        return self.list_of_possible_actions\n",
    "\n",
    "    def _update_reward(self, reward):\n",
    "        # Updates the current reward\n",
    "        self.reward = reward\n",
    "\n",
    "    def _choose_best_action(self, current_state):\n",
    "        self._create_new_state_entries_if_needed(current_state)\n",
    "        maximum_q_values_idx = np.where(self.q_table[current_state][self.list_of_possible_actions] == np.max(self.q_table[current_state][self.list_of_possible_actions]))[0]\n",
    "        random_max = np.random.choice(maximum_q_values_idx)\n",
    "        return (self.list_of_possible_actions[random_max])\n",
    "\n",
    "    def _choose_random_action(self):\n",
    "        return np.random.choice(self.list_of_possible_actions)\n",
    "\n",
    "    def _create_new_state_entries_if_needed(self, state):\n",
    "        if (not isinstance(self.q_table.get(state), np.ndarray)):\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_state_key(board):\n",
    "     # Convert the Board configuration (Matrix) into a unique key for the state\n",
    "        return hashlib.sha1(board).hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72d45b",
   "metadata": {},
   "source": [
    "We implemented a set of utility functions that will be used in the RL training procedure. Specifically:\n",
    "\n",
    "1. A `logger` function that prints the outcome of the game. \n",
    "2. A `choose_players` function that will be used to switch the first player to start to play at every episode.\n",
    "3. A `plot_rewards` function that plot the averaged rewards in a default window of $250$ games.    \n",
    "4. A `plot_metrics` function that plot the $M_{opt}$ (optimal metric) and the $M_{rand}$ (random metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logger(winner, player_1, player_2):\n",
    "    # Logs the information if specified\n",
    "    print('-------------------------------------------')\n",
    "    print(f'Game end, winner is player {str(winner)}')\n",
    "    print(f'Optimal player 1 = {str(player_1)}')\n",
    "    print(f'RL Agent player 2 = {str(player_2)}')\n",
    "    \n",
    "def choose_players(index):\n",
    "    if index%2 == 0:\n",
    "        player_1 = 'X'\n",
    "        player_2 = 'O'\n",
    "    else:\n",
    "        player_1 = 'O'\n",
    "        player_2 = 'X' \n",
    "        \n",
    "    return player_1,player_2     \n",
    "\n",
    "def choose_self_learning_player(step: int):\n",
    "    if step%2 == 0:\n",
    "        return \"X\"\n",
    "    else:\n",
    "        return \"O\"\n",
    "\n",
    "def plot_rewards(number_of_episodes, rewards, ax, label = \"\", plot_every=250):\n",
    "    episodes = np.arange(0,number_of_episodes)\n",
    "    bin_means, _, _ = binned_statistic(episodes, rewards, statistic=\"mean\", bins =(number_of_episodes)/plot_every, range=(0, number_of_episodes))\n",
    "    ax.plot(np.arange(0, number_of_episodes, plot_every), bin_means, label = label)\n",
    "    ax.legend()\n",
    "    return\n",
    "\n",
    "def plot_metrics(number_of_episodes,metrics, ax, label):\n",
    "    ax.plot(np.arange(0, number_of_episodes, 250), metrics, label = label)\n",
    "    ax.legend()\n",
    "    return\n",
    "\n",
    "def convert_value_to_play_character(state: np.ndarray, q_values):\n",
    "    representation: List[List[str]] = [] \n",
    "    line: List[str] = [] \n",
    "    state = state.flatten()\n",
    "\n",
    "    for start in range(0, 9, 3):\n",
    "        line = []\n",
    "        for index in range(start, start+3):\n",
    "            if state[index] == 0:\n",
    "                line.append(f\"{round(float(q_values[index]) ,3)}\")\n",
    "            elif state[index] == 1:\n",
    "                line.append(\"X\")\n",
    "            elif state[index] == -1:\n",
    "                line.append(\"O\")\n",
    "        representation.append(line)\n",
    "    representation = np.array(representation)\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc958b",
   "metadata": {},
   "source": [
    "Now after implementing the needed functionalities of the RL agent we proceed to implementing a RL training procedure for Tic Tac Toe Player using Q-learning algorithm. With that, we will create a Q-table representing all the states and actions possible and we will progressively update the values in the table. The values in this table will depict the reward the agent expects to get if the corresponding action was chosen in the given state. In other words, if the values in the Q-table are the true values, then the optimal policy of the agent is to choose the actions whose corresponding Q-values are maximum in a given state. \n",
    "\n",
    "Since the Q-learning utilize current state, current action and next state Q-values, the Q-table is updated every 2 moves. With that, the Q-learning pipeline will look as follows:\n",
    "\n",
    "Given a certain current Tic-Tac-Toe state denoted by (S):\n",
    "\n",
    "1. RL agent choose an action (A) based on epsilon greedy and stores the chosen action. \n",
    "2. Optimal player choose an action based on the optimal level chosen.\n",
    "\n",
    "Now the RL agent observes a different state denoted by (S')\n",
    "\n",
    "3. RL agent choose an action (A') based on epsilon greedy and update the Q-table using the (S, A, S' and greedy action A*). A* might be different than A' as Q-learning is an-off policy strategy where A* is the action that maximize the Q-value for the given state (S').    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183541ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent_qlearning(environment: TictactoeEnv, number_of_episodes: int, optimal_level : float, epsilon:float, test_episode:int, verbose: bool = False):\n",
    "    # Initialize the Rewards and Test Metrics\n",
    "    rewards = np.zeros(number_of_episodes)\n",
    "    metrics_opt = np.zeros(int(number_of_episodes/test_episode))\n",
    "    metrics_rand = np.zeros(int(number_of_episodes/test_episode))\n",
    "\n",
    "    # Instantiate the Players\n",
    "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
    "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"O\")\n",
    "\n",
    "    for episode in tqdm(range(number_of_episodes)):\n",
    "        player_rl_agent.train()\n",
    "        \n",
    "        # Reset the Environment\n",
    "        environment.reset()\n",
    "        \n",
    "        #Observe the Environment\n",
    "        grid, _, _ = environment.observe()\n",
    "\n",
    "        #Choose the players X,O\n",
    "        optimal_player_character,rl_player_character = choose_players(index = episode)\n",
    "        player_optimal.player = optimal_player_character\n",
    "        player_rl_agent.player = rl_player_character\n",
    "        \n",
    "        # Give RL access to the board\n",
    "        player_rl_agent.observe_state(grid)\n",
    "\n",
    "        # Update nb of episode played\n",
    "        player_rl_agent.update_nb_of_episode_played(episode) \n",
    "\n",
    "        # Number of RL movements \n",
    "        number_of_rl_movements = 0\n",
    "        \n",
    "        for step in range(9):\n",
    "            if environment.current_player == player_optimal.player:\n",
    "                move = player_optimal.act(grid)\n",
    "            \n",
    "            else:\n",
    "                if (number_of_rl_movements%2 ==0):\n",
    "                    # store current state and action\n",
    "                    player_rl_agent.observe_state(grid)\n",
    "                    rl_current_state = grid\n",
    "                    rl_current_action = player_rl_agent.act(grid)\n",
    "        \n",
    "                    move = (int(rl_current_action/3),rl_current_action%3)\n",
    "\n",
    "                else:\n",
    "                    # store next state and action\n",
    "                    player_rl_agent.observe_state(grid)\n",
    "                    rl_next_state = grid\n",
    "                    rl_next_action = player_rl_agent.act(grid)\n",
    "        \n",
    "                    move = (int(rl_next_action/3),rl_next_action%3)\n",
    "\n",
    "                    # update the q-table\n",
    "                    player_rl_agent.update_q_table(rl_current_state, rl_current_action, rl_next_state)\n",
    "\n",
    "                    # set the next state to current state after update\n",
    "                    rl_current_state = rl_next_state\n",
    "                    rl_current_action = rl_next_action\n",
    "\n",
    "                number_of_rl_movements+=1\n",
    "\n",
    "        \n",
    "            grid, end, winner = environment.step(move, print_grid=False)\n",
    "            player_rl_agent.observe_reward(environment.reward(rl_player_character))\n",
    "            \n",
    "            if end:\n",
    "                rewards[episode] = (environment.reward(rl_player_character))\n",
    "\n",
    "                player_rl_agent.update_q_table(rl_current_state, rl_current_action, grid, terminal_state=True )\n",
    "        \n",
    "                if verbose:\n",
    "                    logger(winner, optimal_player_character, rl_player_character)\n",
    "                    environment.render()\n",
    "                environment.reset()\n",
    "                break\n",
    "        \n",
    "        if (episode%test_episode == 0):\n",
    "            m_opt, m_rand = compute_metrics(environment, player_rl_agent, 500)\n",
    "            metrics_opt[int(episode/test_episode)] = m_opt\n",
    "            metrics_rand[int(episode/test_episode)] = m_rand\n",
    "    return player_rl_agent, rewards, metrics_opt, metrics_rand   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169da625",
   "metadata": {},
   "source": [
    "In order to assess the \"test\" performance of the RL agent, we will need to compute the M_opt and M_rand metric. The M_opt metric assesses how well the RL agent performs against the player following an optimal policy while the M_rand metric assesses how well the RL agent performs against a player playing a random policy.\n",
    "\n",
    "With that, two functions are needed. A function that allows the RL agent to play (without updating the Q_table) against the above described players and anoother function that computes the $M_{opt}$ and $M_{rand}$ using the equation:\n",
    "$ (N_{win} - N_{loss})/ N_{played} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(environment: TictactoeEnv, player_rl_agent:RlAgent, number_of_episodes:int, verbose: bool = False):\n",
    "    player_rl_agent.test()\n",
    "    \n",
    "    number_of_rl_wins, number_of_optimal_wins = evaluate_rl_agent_qlearning(environment, player_rl_agent,number_of_episodes, 0.0, verbose)\n",
    "    M_opt = (number_of_rl_wins -  number_of_optimal_wins)/number_of_episodes\n",
    "\n",
    "    number_of_rl_wins, number_of_optimal_wins = evaluate_rl_agent_qlearning(environment, player_rl_agent,number_of_episodes, 1.0, verbose)\n",
    "    M_rand = (number_of_rl_wins -  number_of_optimal_wins)/number_of_episodes\n",
    "\n",
    "    return M_opt, M_rand\n",
    "\n",
    "def evaluate_rl_agent_qlearning(environment: TictactoeEnv, player_rl_agent:RlAgent, number_of_episodes:int, optimal_level : float, verbose: bool = False):\n",
    "    # Instantiate the Players\n",
    "    player_optimal = OptimalPlayer(epsilon=optimal_level, player=\"X\")\n",
    "    player_rl_agent.player = \"O\"\n",
    "\n",
    "    number_of_rl_wins = 0\n",
    "    number_of_optimal_wins = 0\n",
    "    for episode in range(number_of_episodes):\n",
    "        # Reset the Environment\n",
    "        environment.reset()\n",
    "        \n",
    "        #Observe the Environment\n",
    "        grid, _, _ = environment.observe()\n",
    "\n",
    "        #Choose the players X,O\n",
    "        optimal_player_character, rl_player_character = choose_players(index = episode)\n",
    "        player_optimal.player = optimal_player_character\n",
    "        player_rl_agent.player = rl_player_character\n",
    "        \n",
    "        # Give RL access to the board\n",
    "        player_rl_agent.observe_state(grid)\n",
    "        \n",
    "        for step in range(9):\n",
    "            if environment.current_player == player_optimal.player:\n",
    "                move = player_optimal.act(grid)\n",
    "            else:\n",
    "                player_rl_agent.observe_state(grid)\n",
    "                rl_current_action = player_rl_agent.act(grid)\n",
    "                move = (int(rl_current_action/3),rl_current_action%3)\n",
    "        \n",
    "            grid, end, winner = environment.step(move, print_grid=False)\n",
    "            \n",
    "            \n",
    "            if end:\n",
    "                if winner == optimal_player_character:\n",
    "                    number_of_optimal_wins +=1\n",
    "                    \n",
    "                if winner == rl_player_character:\n",
    "                    number_of_rl_wins += 1\n",
    "                    \n",
    "                if verbose:\n",
    "                    logger(winner, optimal_player_character, rl_player_character)\n",
    "                    environment.render()\n",
    "                environment.reset()\n",
    "                break\n",
    "        \n",
    "\n",
    "    return number_of_rl_wins, number_of_optimal_wins   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e5948",
   "metadata": {},
   "source": [
    "Finally, after implementing all the needed functions, we will now explore the effects of the epsilon on training. We will start by using a fixed epsilon through the training procedure (i.e accross all the episodes) and then we will compare the training preformance with that of a monotonically decreasing epsilon through out the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58009e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with a fixed epsilon (Question 1)\n",
    "\n",
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "\n",
    "# Environment\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "# plots \n",
    "fig,ax = plt.subplots()\n",
    "plt.title(\"Training with fixed epsilon\")\n",
    "\n",
    "# List of epsilons to try \n",
    "epsilons= [0.0, 0.1, 0.5, 0.9]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Train and get the rewards for a fixed epsilon \n",
    "    player_rl_agent, rewards, _, _ = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=0.5, epsilon=epsilon, test_episode=number_of_episodes)\n",
    "    plot_rewards(number_of_episodes, rewards, ax, label=f\"Fixed Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with a monotonically decreasing epsilon (Question 2 and Question 3)\n",
    "\n",
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "epsilon_min, epsilon_max = 0.1, 0.8\n",
    "\n",
    "# Environment\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "# Plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "axes[0].set_title(\"Rewards\")\n",
    "axes[1].set_title(\"Optimal Metric\")\n",
    "axes[2].set_title(\"Random Metric\")\n",
    "\n",
    "list_of_number_of_exploratory_games = [1, 5000, 15000, 30000, 40000]\n",
    " \n",
    "for number_of_exploratory_games in list_of_number_of_exploratory_games:\n",
    "    # Train and get the rewards, M_opt, M_rand for a decreasing monotinically epsilon \n",
    "    epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
    "    player_rl_agent, rewards, m_opt, m_rand = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=0.5, epsilon=epsilon, test_episode=250)\n",
    "    plot_rewards(number_of_episodes,rewards, axes[0], label = f\"n* = {number_of_exploratory_games} \")\n",
    "    plot_metrics(number_of_episodes,m_opt, axes[1], label = f\"n* = {number_of_exploratory_games} \")\n",
    "    plot_metrics(number_of_episodes,m_rand, axes[2], label = f\"n* = {number_of_exploratory_games} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca22352",
   "metadata": {},
   "source": [
    "Now that we have explored the effect of the (fixed/varying) epsilon on the training performance, we will explore the effect of the optimality of the teacher on the training policy. Specifically, would the RL agent train better if it trained against an optimal policy or a random policy or somewhere in between?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb15418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with various Optimal level of the Agent for the optimal n* = 5000 (Question 4). \n",
    "\n",
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "epsilon_min,epsilon_max = 0.1, 0.8\n",
    "number_of_exploratory_games = 5000\n",
    "epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
    "\n",
    "# Environment\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,6))\n",
    "axes[0].set_title(\"Rewards\")\n",
    "axes[1].set_title(\"Optimal Metric\")\n",
    "axes[2].set_title(\"Random Metric\")\n",
    "\n",
    "list_of_optimal_levels = [0,0.25,0.5,0.75,1]\n",
    "for optimal_level in list_of_optimal_levels:\n",
    "    # Train and get the rewards, M_opt, M_rand for a different optimality levels of the teacher  \n",
    "    player_rl_agent, rewards, m_opt, m_rand = train_rl_agent_qlearning(environment, number_of_episodes=number_of_episodes, optimal_level=optimal_level,epsilon=epsilon, test_episode=250)\n",
    "    plot_rewards(number_of_episodes,rewards, axes[0], label = f\"epsilon-Opt = {optimal_level} \")\n",
    "    plot_metrics(number_of_episodes,m_opt, axes[1], label = f\"epsilon-Opt = {optimal_level} \")\n",
    "    plot_metrics(number_of_episodes,m_rand, axes[2], label = f\"epsilon-Opt = {optimal_level} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217deb1",
   "metadata": {},
   "source": [
    "Now that we have deeply explored the performance the effect of the epsilon (exploration probability) and the optimality level of teacher. It would be interresting to see weather the RL Agent can learn by playing against it self since this capability is highly valued when we don't know what the optimal policy is (very common in real-life problems). \n",
    "\n",
    "With that, we implemented a new training procedure, where the RL agent plays againt it self by choosing actions from and updating the values of the same Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent_self_learning(environment: TictactoeEnv, number_of_episodes: int, epsilon:float, test_episode:int, verbose: bool = False):\n",
    "    # Initialize the Rewards and Test Metrics\n",
    "    metrics_opt = np.zeros(int(number_of_episodes/test_episode))\n",
    "    metrics_rand = np.zeros(int(number_of_episodes/test_episode))\n",
    "\n",
    "    # Instantiate the Players\n",
    "    player_rl_agent = RlAgent(epsilon=epsilon, player=\"X\")\n",
    "\n",
    "    for episode in tqdm(range(number_of_episodes)):\n",
    "        player_rl_agent.train()\n",
    "        \n",
    "        # Reset the Environment\n",
    "        environment.reset()\n",
    "        \n",
    "        #Observe the Environment\n",
    "        grid, _, _ = environment.observe()\n",
    "        \n",
    "        # Give RL access to the board\n",
    "        player_rl_agent.observe_state(grid)\n",
    "\n",
    "        # Update nb of episode played\n",
    "        player_rl_agent.update_nb_of_episode_played(episode) \n",
    "\n",
    "        # Number of RL movements \n",
    "        number_of_rl_movements = 0\n",
    "        \n",
    "        player_rl_agent.player = choose_self_learning_player(episode)\n",
    "        for step in range(9):\n",
    "            # if the RL player 1 first move, store player 1 current state and play player 1 current action \n",
    "            if (number_of_rl_movements == 0):\n",
    "                rl_player_1_current_state = grid\n",
    "                rl_player_1_current_action = player_rl_agent.act(grid)\n",
    "                move = (int(rl_player_1_current_action/3),rl_player_1_current_action%3) \n",
    "\n",
    "                number_of_rl_movements=1\n",
    "\n",
    "            # if the RL player 2 first move, store player 2 current state and play player 2 current action\n",
    "            elif (number_of_rl_movements == 1):\n",
    "                rl_player_2_current_state = grid\n",
    "                rl_player_2_current_action = player_rl_agent.act(grid)\n",
    "                move = (int(rl_player_2_current_action/3),rl_player_2_current_action%3) \n",
    "\n",
    "                number_of_rl_movements=2\n",
    "\n",
    "            # if the RL player 1 second move, update player RL q-table \n",
    "            elif (number_of_rl_movements == 2):\n",
    "                rl_player_1_next_state = grid\n",
    "                rl_player_1_next_action = player_rl_agent.act(grid)\n",
    "                move = (int(rl_player_1_next_action/3),rl_player_1_next_action%3)\n",
    "                player_rl_agent.update_q_table(rl_player_1_current_state, rl_player_1_current_action, rl_player_1_next_state)\n",
    "\n",
    "                rl_player_1_current_state = rl_player_1_next_state\n",
    "                rl_player_1_current_action = rl_player_1_next_action\n",
    "                number_of_rl_movements = 3\n",
    "\n",
    "            # if the RL player 2 second move, update player RL q-table \n",
    "            elif (number_of_rl_movements == 3):\n",
    "                rl_player_2_next_state = grid\n",
    "                rl_player_2_next_action = player_rl_agent.act(grid)\n",
    "                move = (int(rl_player_2_next_action/3),rl_player_2_next_action%3)\n",
    "                player_rl_agent.update_q_table(rl_player_2_current_state, rl_player_2_current_action, rl_player_2_next_state)\n",
    "\n",
    "                rl_player_2_current_state = rl_player_2_next_state\n",
    "                rl_player_2_current_action = rl_player_2_next_action\n",
    "                \n",
    "                number_of_rl_movements = 0\n",
    "                                \n",
    "\n",
    "            grid, end, _ = environment.step(move, print_grid=False)\n",
    "            player_rl_agent.observe_state(grid, store_state=True)\n",
    "            player_rl_agent.observe_reward(environment.reward(player_rl_agent.player))\n",
    "            \n",
    "            if end:                \n",
    "                if player_rl_agent.player == \"X\":\n",
    "                    player_rl_agent.update_q_table(rl_player_1_current_state, rl_player_1_current_action, grid, terminal_state=True)\n",
    "                else:\n",
    "                    player_rl_agent.update_q_table(rl_player_2_current_state, rl_player_2_current_action, grid, terminal_state=True)\n",
    "                environment.reset()\n",
    "                break\n",
    "        \n",
    "        if (episode%test_episode == 0):\n",
    "            m_opt, m_rand = compute_metrics(environment, player_rl_agent, 500)\n",
    "            metrics_opt[int(episode/test_episode)] = m_opt\n",
    "            metrics_rand[int(episode/test_episode)] = m_rand\n",
    "    return player_rl_agent, metrics_opt, metrics_rand   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e56f0",
   "metadata": {},
   "source": [
    "With that, we will proceed as before by evaluating the effect of fixed/varying epsilon on the self-learning training procedure by allowing the RL agent play against it self and updating the same Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training by playing against it self with fixed epsilon (Question 7)\n",
    "\n",
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "\n",
    "# Environment\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "# plots \n",
    "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
    "axes[0].set_title(\"Optimal Metric\")\n",
    "axes[1].set_title(\"Random Metric\")\n",
    "\n",
    "\n",
    "list_of_epsilons = [0.1,0.5,0.9]\n",
    "for epsilon in list_of_epsilons:\n",
    "# Train and get the M_opt, and M_rand for a fixed epsilon\n",
    "    player_rl_agent, m_opt, m_rand = train_rl_agent_self_learning(environment, number_of_episodes=number_of_episodes,epsilon=epsilon, test_episode=250)\n",
    "    plot_metrics(number_of_episodes,m_opt, axes[0],label=f\"Fixed Epsilon: {epsilon}\")\n",
    "    plot_metrics(number_of_episodes,m_rand, axes[1],label=f\"Fixed Epsilon: {epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training by playing against it self with decreasing epsilon (Question 8)\n",
    "\n",
    "# RL Hyper-params\n",
    "number_of_episodes = 20000\n",
    "epsilon_min,epsilon_max = 0.1, 0.8\n",
    "\n",
    "# Environment\n",
    "environment = TictactoeEnv()\n",
    "\n",
    "# plots \n",
    "fig, axes = plt.subplots(1,2,figsize=(16,6))\n",
    "axes[0].set_title(\"Optimal Metric\")\n",
    "axes[1].set_title(\"Random Metric\")\n",
    "\n",
    "list_of_number_of_exploratory_games = [1,5000,15000,30000,40000]\n",
    "\n",
    "# Define the best player agent \n",
    "best_player_agent = None\n",
    "best_m_opt, best_m_rand = -1, -1 \n",
    "for number_of_exploratory_games in list_of_number_of_exploratory_games:\n",
    "# Train and get the M_opt and M_rand for a number of exploratory games\n",
    "    epsilon = lambda n: max(epsilon_min, epsilon_max*(1-(n/number_of_exploratory_games)))\n",
    "    player_rl_agent, m_opt, m_rand = train_rl_agent_self_learning(environment, number_of_episodes=number_of_episodes,epsilon=epsilon, test_episode=250)\n",
    "    \n",
    "    if m_opt[-1]>best_m_opt and m_rand[-1]>best_m_rand:\n",
    "        best_player_agent = player_rl_agent\n",
    "        best_m_opt, best_m_rand = m_opt[-1], m_rand[-1]\n",
    "\n",
    "    plot_metrics(number_of_episodes,m_opt, axes[0],label= f\"n* = {number_of_exploratory_games} \")\n",
    "    plot_metrics(number_of_episodes,m_rand, axes[1],label= f\"n* = {number_of_exploratory_games} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb00b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best player agent\n",
    "player_rl_agent = best_player_agent\n",
    "\n",
    "# Visualize the heat map\n",
    "number_of_visualized_states = 3\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(16,6))\n",
    "for index in range(number_of_visualized_states):\n",
    "    state = player_rl_agent.observed_states[randrange(len(player_rl_agent.observed_states))]\n",
    "    state_key = player_rl_agent.get_state_key(state)\n",
    "    q_values = player_rl_agent.q_table[state_key]\n",
    "    q_values_reshaped = q_values.reshape((3,3))\n",
    "    sns.heatmap(q_values_reshaped, annot = convert_value_to_play_character(state, q_values), ax=ax[index], fmt=\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
